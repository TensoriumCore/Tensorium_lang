================================================================================
FILE: lib/CMakeLists.txt
================================================================================
add_library(tensorium
  AST/ASTPrinter.cpp
  AST/Expr.cpp
  Backend/BackendBuilder.cpp
  Lex/Lexer.cpp
  Parse/Parser.cpp
  Runtime/CpuRuntime.cpp
  Runtime/Eval.cpp
  Sema/Sema.cpp
  Sema/ProgramValidator.cpp
)

target_include_directories(tensorium PUBLIC
  ${PROJECT_SOURCE_DIR}/include
)

set(LLVM_TARGET_DEFINITIONS
  ${PROJECT_SOURCE_DIR}/include/tensorium_mlir/Dialect/Tensorium/Transform/Passes.td
)

mlir_tablegen(TensoriumPasses.h.inc
  -gen-pass-decls
  -name Tensorium
  -I ${PROJECT_SOURCE_DIR}/include
  -I ${MLIR_INCLUDE_DIRS}
  -I ${LLVM_INCLUDE_DIRS}
)

add_public_tablegen_target(TensoriumPassesIncGen)

set(LLVM_TARGET_DEFINITIONS
  ${PROJECT_SOURCE_DIR}/include/tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.td
)

mlir_tablegen(TensoriumOps.h.inc
  -gen-op-decls
  -I ${PROJECT_SOURCE_DIR}/include
  -I ${MLIR_INCLUDE_DIRS}
  -I ${LLVM_INCLUDE_DIRS}
)

mlir_tablegen(TensoriumOps.cpp.inc
  -gen-op-defs
  -I ${PROJECT_SOURCE_DIR}/include
  -I ${MLIR_INCLUDE_DIRS}
  -I ${LLVM_INCLUDE_DIRS}
)

add_public_tablegen_target(TensoriumOpsIncGen)

add_library(tensorium_mlir_backend
  tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.cpp
  tensorium_mlir/Dialect/Tensorium/IR/TensoriumTypes.cpp
  tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.cpp
  tensorium_mlir/Dialect/Tensorium/Transforms/AnalysisPass.cpp
  tensorium_mlir/Dialect/Tensorium/Transforms/NoOpPass.cpp
  tensorium_mlir/Dialect/Tensorium/Transforms/Passes.cpp
  tensorium_mlir/Target/MLIRGen/MLIRGen.cpp
  tensorium_mlir/Init/Registry.cpp
  tensorium_mlir/Init/Passes.cpp
  tensorium_mlir/Pipeline/Pipeline.cpp
  tensorium_mlir/Dialect/Tensorium/Transforms/EinsteinLoweringPass.cpp
  tensorium_mlir/Dialect/Tensorium/Transforms/IndexRoleAnalysisPass.cpp
  tensorium_mlir/Dialect/Tensorium/Transforms/EinsteinValidityPass.cpp
  tensorium_mlir/Dialect/Tensorium/Transforms/IndexAnalyzePass.cpp
  tensorium_mlir/Dialect/Tensorium/Transforms/EinsteinCanonicalizePass.cpp
  tensorium_mlir/Semantic/Einstein.cpp
  tensorium_mlir/Dialect/Tensorium/Transforms/EinsteinAnalyzeEinsumPass.cpp
)

add_dependencies(tensorium_mlir_backend
  TensoriumPassesIncGen
  TensoriumOpsIncGen
)

target_include_directories(tensorium_mlir_backend SYSTEM PRIVATE
  ${PROJECT_SOURCE_DIR}/include
  ${PROJECT_BINARY_DIR}/lib
  ${CMAKE_CURRENT_BINARY_DIR} 
  ${LLVM_INCLUDE_DIRS}
  ${MLIR_INCLUDE_DIRS}
)

# target_include_directories(tensorium_mlir_backend SYSTEM PRIVATE
#   ${LLVM_INCLUDE_DIRS}
#   ${MLIR_INCLUDE_DIRS}
# )

set_target_properties(tensorium_mlir_backend PROPERTIES
  CXX_STANDARD 20
  POSITION_INDEPENDENT_CODE ON
)

target_link_libraries(tensorium_mlir_backend PRIVATE
  MLIRIR
  MLIRSupport
  MLIRPass
  MLIRTransforms
  MLIRFuncDialect
  MLIRArithDialect
  MLIRSCFDialect
  MLIRMemRefDialect
  MLIRMathDialect
  MLIRInferTypeOpInterface
  MLIRBytecodeOpInterface
)


================================================================================
FILE: lib/tensorium_mlir/Init/Passes.cpp
================================================================================
#include "tensorium_mlir/Init/Passes.h"
#include "tensorium_mlir/Dialect/Tensorium/Transform/EinsteinLoweringPass.h"
namespace tensorium_mlir {

void registerAllPasses() {
  // No Tensorium passes registered yet.
  // This is intentionally empty for now.
}

} // namespace tensorium_mlir


================================================================================
FILE: lib/tensorium_mlir/Init/Registry.cpp
================================================================================
#include "tensorium_mlir/Init/Registry.h"

#include "mlir/Dialect/Arith/IR/Arith.h"
#include "mlir/Dialect/Func/IR/FuncOps.h"
#include "mlir/Dialect/Math/IR/Math.h"
#include "mlir/Dialect/MemRef/IR/MemRef.h"
#include "mlir/Dialect/SCF/IR/SCF.h"

namespace tensorium_mlir {

void registerAllDialects(mlir::DialectRegistry &registry) {
  registry.insert<mlir::func::FuncDialect, mlir::arith::ArithDialect,
                  mlir::scf::SCFDialect, mlir::memref::MemRefDialect,
                  mlir::math::MathDialect>();
}

} // namespace tensorium_mlir


================================================================================
FILE: lib/tensorium_mlir/Pipeline/Pipeline.cpp
================================================================================
#include "tensorium_mlir/Pipeline/Pipeline.h"

#include "mlir/IR/Verifier.h"
#include "mlir/Pass/PassManager.h"
#include "mlir/Transforms/Passes.h"

#include "tensorium_mlir/Dialect/Tensorium/Transform/Passes.h"

namespace tensorium_mlir {

void buildDefaultPipeline(mlir::PassManager &pm, const PipelineOptions &opt) {
  pm.enableVerifier(opt.verify);

  if (opt.printIR) {
    pm.enableIRPrinting(
        /*shouldPrintBeforePass=*/[](mlir::Pass *,
                                     mlir::Operation *) { return true; },
        /*shouldPrintAfterPass=*/
        [](mlir::Pass *, mlir::Operation *) { return true; },
        /*printModuleScope=*/true,
        /*printAfterOnlyOnChange=*/false);
  }

  pm.addPass(mlir::createCSEPass());
  pm.addPass(mlir::createCanonicalizerPass());
}

} // namespace tensorium_mlir


================================================================================
FILE: lib/tensorium_mlir/Target/MLIRGen/MLIRGen.cpp
================================================================================
#include "tensorium_mlir/Target/MLIRGen/MLIRGen.h"
#include "mlir/Dialect/Arith/IR/Arith.h"
#include "mlir/Dialect/Func/IR/FuncOps.h"
#include "mlir/IR/Builders.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/IR/MLIRContext.h"
#include "mlir/Pass/PassManager.h"
#include "mlir/Transforms/Passes.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumTypes.h"
#include "tensorium_mlir/Dialect/Tensorium/Transform/Passes.h"
#include "llvm/ADT/SmallVector.h"
#include "llvm/Support/raw_ostream.h"

namespace tensorium_mlir {

namespace {

struct FieldDesc {
  std::string name;
  unsigned rank = 0;
  tensorium::mlir::Variance variance = tensorium::mlir::Variance::Scalar;
};

static mlir::ArrayAttr makeIndexArrayAttr(mlir::OpBuilder &b,
                                          const std::vector<std::string> &idx) {
  llvm::SmallVector<mlir::Attribute, 4> names;
  for (const auto &s : idx)
    names.push_back(b.getStringAttr(s));
  return b.getArrayAttr(names);
}

static bool startsWith(const std::string &s, const char *prefix) {
  size_t n = std::char_traits<char>::length(prefix);
  return s.size() >= n && s.compare(0, n, prefix) == 0;
}

static std::vector<FieldDesc>
extractFields(const tensorium::backend::ModuleIR &module) {
  std::vector<FieldDesc> out;
  for (const auto &f : module.fields) {
    FieldDesc d;
    d.name = f.name;
    d.rank = f.up + f.down;
    if (f.up == 0 && f.down == 0)
      d.variance = tensorium::mlir::Variance::Scalar;
    else if (f.up > 0 && f.down == 0)
      d.variance = tensorium::mlir::Variance::Contravariant;
    else if (f.up == 0 && f.down > 0)
      d.variance = tensorium::mlir::Variance::Covariant;
    else
      d.variance = tensorium::mlir::Variance::Mixed;
    out.push_back(std::move(d));
  }
  return out;
}

static mlir::Value
emitExpr(mlir::OpBuilder &b, mlir::Location loc, mlir::Type f64,
         const tensorium::backend::ExprIR *e,
         const llvm::DenseMap<llvm::StringRef, mlir::Value> &fieldArg) {
  using namespace tensorium::backend;
  if (!e)
    return {};

  switch (e->kind) {
  case ExprIR::Kind::Number: {
    auto *n = static_cast<const NumberIR *>(e);
    return b.create<tensorium::mlir::ConstOp>(loc, f64,
                                              b.getF64FloatAttr(n->value));
  }
  case ExprIR::Kind::Var: {
    auto *v = static_cast<const VarIR *>(e);
    auto it = fieldArg.find(v->name);
    if (it == fieldArg.end())
      return {};

    auto r = b.create<tensorium::mlir::RefOp>(loc, f64, it->second,
                                              b.getStringAttr("field"));
    if (!v->tensorIndexNames.empty()) {
      llvm::SmallVector<mlir::Attribute, 4> idxAttr;
      for (const auto &s : v->tensorIndexNames)
        idxAttr.push_back(b.getStringAttr(s));
      r->setAttr("indices", b.getArrayAttr(idxAttr));
    }
    return r.getResult();
  }
  case ExprIR::Kind::Binary: {
    auto *bin = static_cast<const BinaryIR *>(e);
    auto L = emitExpr(b, loc, f64, bin->lhs.get(), fieldArg);
    auto R = emitExpr(b, loc, f64, bin->rhs.get(), fieldArg);
    if (!L || !R)
      return {};

    if (bin->op == "+")
      return b.create<tensorium::mlir::AddOp>(loc, f64, L, R);
    if (bin->op == "*")
      return b.create<tensorium::mlir::MulOp>(loc, f64, L, R);
    if (bin->op == "-")
      return b.create<tensorium::mlir::SubOp>(loc, f64, L, R);
    return {};
  }
  case ExprIR::Kind::Call: {
    auto *c = static_cast<const CallIR *>(e);
    if (startsWith(c->callee, "d_") && c->callee.size() == 3) {
      if (c->args.empty())
        return {};
      auto arg0 = emitExpr(b, loc, f64, c->args[0].get(), fieldArg);
      auto deriv =
          b.create<tensorium::mlir::DerivOp>(loc, arg0.getType(), arg0);
      deriv->setAttr("index", b.getStringAttr(std::string(1, c->callee[2])));
      return deriv.getResult();
    }
    if (c->callee == "contract") {
      if (c->args.empty())
        return {};
      auto arg0 = emitExpr(b, loc, f64, c->args[0].get(), fieldArg);
      return b.create<tensorium::mlir::ContractOp>(loc, f64, arg0);
    }
    return {};
  }
  default:
    return {};
  }
}
} // namespace

static void addEinsteinPipelineSafe(::mlir::PassManager &pm,
                                    const MLIRGenOptions &opts) {

  if (opts.enableEinsteinLoweringPass) {
    pm.addPass(tensorium::mlir::createTensoriumEinsteinLoweringPass());
  }

  const bool needValidity = opts.enableEinsteinValidityPass;
  const bool needCanon = opts.enableEinsteinCanonicalizePass;
  const bool needAnalyze = opts.enableEinsteinAnalyzeEinsumPass || needValidity;
  const bool needIndex = opts.enableIndexAnalyzePass || needValidity;

  if (needIndex) {
    pm.addPass(tensorium::mlir::createTensoriumIndexAnalyzePass());
  }

  if (needAnalyze) {
    pm.addPass(tensorium::mlir::createTensoriumEinsteinAnalyzeEinsumPass());
  }

  if (needCanon) {
    pm.addPass(tensorium::mlir::createTensoriumEinsteinCanonicalizePass());
  }

  if (needValidity) {
    pm.addPass(tensorium::mlir::createTensoriumEinsteinValidityPass());
  }
}

void emitMLIR(const tensorium::backend::ModuleIR &module,
              const MLIRGenOptions &opts) {
  mlir::MLIRContext ctx;
  ctx.getOrLoadDialect<mlir::func::FuncDialect>();
  ctx.getOrLoadDialect<mlir::arith::ArithDialect>();
  ctx.getOrLoadDialect<tensorium::mlir::TensoriumDialect>();

  mlir::OpBuilder b(&ctx);
  auto loc = b.getUnknownLoc();
  auto moduleOp = mlir::ModuleOp::create(loc);

  const auto fields = extractFields(module);
  llvm::SmallVector<mlir::Type, 8> argTypes;
  for (const auto &fd : fields) {
    argTypes.push_back(tensorium::mlir::FieldType::get(&ctx, b.getF64Type(),
                                                       fd.rank, fd.variance));
  }

  auto funcTy = b.getFunctionType(argTypes, {});
  auto f = b.create<mlir::func::FuncOp>(loc, "tensorium_entry", funcTy);
  auto *entry = f.addEntryBlock();
  b.setInsertionPointToEnd(entry);

  llvm::DenseMap<llvm::StringRef, mlir::Value> fieldArg;
  for (unsigned i = 0; i < fields.size(); ++i) {
    fieldArg[fields[i].name] = entry->getArgument(i);
  }

  auto f64 = b.getF64Type();
  for (const auto &evo : module.evolutions) {
    for (const auto &eq : evo.equations) {
      auto it = fieldArg.find(eq.fieldName);
      if (it == fieldArg.end())
        continue;
      auto rhsV = emitExpr(b, loc, f64, eq.rhs.get(), fieldArg);
      if (!rhsV)
        continue;
      b.create<tensorium::mlir::DtAssignOp>(loc, it->second, rhsV,
                                            makeIndexArrayAttr(b, eq.indices));
    }
  }
  b.create<mlir::func::ReturnOp>(loc);
  moduleOp.push_back(f);

  mlir::PassManager pm(&ctx);
  addEinsteinPipelineSafe(pm, opts);

  pm.addPass(mlir::createCanonicalizerPass());
  pm.addPass(mlir::createCSEPass());
  if (mlir::failed(pm.run(moduleOp))) {
    llvm::errs() << "Pipeline failed\n";
  }
  moduleOp.print(llvm::outs());
}

} // namespace tensorium_mlir


================================================================================
FILE: lib/tensorium_mlir/Conversion/TensoriumToLinalg/TensoriumToLinalg.cpp
================================================================================


================================================================================
FILE: lib/tensorium_mlir/Semantic/Einstein.cpp
================================================================================

#include "tensorium_mlir/Semantic/Einstein.h"
#include "llvm/ADT/ArrayRef.h"
#include "llvm/ADT/DenseSet.h"
#include "llvm/ADT/SmallVector.h"

namespace tensorium::semantic {

llvm::StringRef roleToString(IndexRoleKind r) {
  switch (r) {
  case IndexRoleKind::Free:
    return "free";
  case IndexRoleKind::Contracted:
    return "contracted";
  case IndexRoleKind::Summed:
    return "summed";
  case IndexRoleKind::Dangling:
    return "dangling";
  case IndexRoleKind::Invalid:
    return "invalid";
  }
  return "invalid";
}

static llvm::SmallVector<llvm::StringRef, 16>
computeAllSorted(llvm::ArrayRef<llvm::SmallVector<llvm::StringRef, 8>> ins,
                 llvm::ArrayRef<llvm::StringRef> out) {
  llvm::SmallVector<llvm::StringRef, 16> all;
  llvm::DenseSet<llvm::StringRef> seen;

  for (auto x : out)
    if (seen.insert(x).second)
      all.push_back(x);

  for (auto &vec : ins)
    for (auto x : vec)
      if (seen.insert(x).second)
        all.push_back(x);

  return all;
}

EinsteinAnalysisResult analyzeEinstein(llvm::ArrayRef<llvm::StringRef> outIdx,
                                       llvm::ArrayRef<llvm::StringRef> rhsIdx,
                                       const EinsteinAnalyzeOptions &opt) {
  EinsteinAnalysisResult res;

  res.out.assign(outIdx.begin(), outIdx.end());
  res.ins.clear();
  res.ins.emplace_back(rhsIdx.begin(), rhsIdx.end());

  for (auto s : rhsIdx)
    res.counts[s] += 1;

  res.all = computeAllSorted(res.ins, res.out);
  llvm::SmallDenseSet<llvm::StringRef, 16> outSet;
  for (auto x : res.out)
    outSet.insert(x);

  res.valid = true;

  for (auto idx : res.all) {
    const int64_t c = res.counts.lookup(idx);
    const bool inOut = outSet.contains(idx);

    IndexRoleKind role = IndexRoleKind::Invalid;

    if (inOut) {
      role = IndexRoleKind::Free;
      if (c != 1) {
        res.valid = false;
        role = IndexRoleKind::Invalid;
      }
    } else {
      if (c == 2) {
        role = IndexRoleKind::Contracted;
      } else if (c > 2) {
        role = IndexRoleKind::Summed;
        if (!opt.allowSummed)
          res.valid = false;
      } else if (c == 1) {
        role = IndexRoleKind::Dangling;
        if (!opt.allowDangling)
          res.valid = false;
      } else {
        role = IndexRoleKind::Invalid;
        res.valid = false;
      }
    }

    res.roles[idx] = role;
  }

  return res;
}

} // namespace tensorium::semantic


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.cpp
================================================================================
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.h"
#include "mlir/IR/Builders.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumTypes.h"

using namespace mlir;
namespace tensorium {
namespace mlir {

LogicalResult IndexOp::verify() {
  auto fieldTy =
      llvm::dyn_cast<tensorium::mlir::FieldType>(getField().getType());
  if (!fieldTy)
    return emitOpError("operand must be a tensorium.field");

  auto idx = getIndices();
  if (!idx)
    return emitOpError("missing indices attribute");

  unsigned rank = fieldTy.getRank();
  unsigned nidx = idx.size();

  if (rank != nidx)
    return emitOpError() << "wrong number of indices: expected " << rank
                         << ", got " << nidx;

  llvm::SmallDenseSet<llvm::StringRef, 16> seen;
  for (Attribute a : idx) {
    auto s = llvm::dyn_cast<StringAttr>(a);
    if (!s)
      return emitOpError("indices must be an array of string attributes");
    auto v = s.getValue();
    if (!seen.insert(v).second)
      return emitOpError() << "duplicate index '" << v << "'";
  }

  return success();
}

ParseResult EinsumOp::parse(OpAsmParser &parser, OperationState &result) {
  SmallVector<OpAsmParser::UnresolvedOperand, 8> operands;
  Type resultType;

  if (parser.parseOperandList(operands))
    return failure();

  if (parser.parseColon())
    return failure();

  if (parser.parseType(resultType))
    return failure();

  SmallVector<Type, 8> operandTypes(operands.size(), resultType);
  if (parser.resolveOperands(operands, operandTypes,
                             parser.getCurrentLocation(), result.operands))
    return failure();

  result.addTypes(resultType);

  if (succeeded(parser.parseOptionalAttrDict(result.attributes)))
    return success();

  return success();
}

void EinsumOp::print(OpAsmPrinter &p) {
  p << " " << getOperands();
  p << " {";
  p.printNewline();
  p.increaseIndent();

  auto printOne = [&](StringRef name) {
    if (auto a = (*this)->getAttr(name)) {
      p.printNewline();
      p << name << " = ";
      p.printAttribute(a);
    }
  };

  printOne("spec");
  printOne("tin.idx.ins");
  printOne("tin.idx.out");
  printOne("tin.idx.all");
  printOne("tin.idx.counts");
  printOne("tin.idx.roles");
  printOne("tin.idx.valid");

  for (auto na : (*this)->getAttrs()) {
    auto n = na.getName().strref();
    if (n == "spec" || n.starts_with("tin.idx."))
      continue;
    p.printNewline();
    p << n << " = ";
    p.printAttribute(na.getValue());
  }

  p.decreaseIndent();
  p.printNewline();
  p << "}";
  p << " : " << getResult().getType();
}
} // namespace mlir
} // namespace tensorium

#define GET_OP_CLASSES
#include "TensoriumOps.cpp.inc"


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.cpp
================================================================================

#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.h"
#include "mlir/IR/DialectImplementation.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumTypes.h"
#include "llvm/ADT/TypeSwitch.h"

using namespace mlir;

namespace tensorium {
namespace mlir {

TensoriumDialect::TensoriumDialect(MLIRContext *ctx)
    : Dialect(getDialectNamespace(), ctx, TypeID::get<TensoriumDialect>()) {
  addTypes<FieldType>();
  addOperations<
#define GET_OP_LIST
#include "TensoriumOps.cpp.inc"
      >();
}

Type TensoriumDialect::parseType(DialectAsmParser &parser) const {
  StringRef tag;
  if (failed(parser.parseKeyword(&tag)))
    return Type();

  if (tag != "field") {
    parser.emitError(parser.getNameLoc(), "unknown tensorium type: ") << tag;
    return Type();
  }

  if (failed(parser.parseLess()))
    return Type();

  Type elementType;
  if (failed(parser.parseType(elementType)))
    return Type();

  if (failed(parser.parseComma()))
    return Type();

  unsigned rank = 0;
  if (failed(parser.parseInteger(rank)))
    return Type();

  if (failed(parser.parseComma()))
    return Type();

  StringRef varTag;
  if (failed(parser.parseKeyword(&varTag)))
    return Type();

  Variance variance;
  if (varTag == "scalar")
    variance = Variance::Scalar;
  else if (varTag == "cov")
    variance = Variance::Covariant;
  else if (varTag == "con")
    variance = Variance::Contravariant;
  else if (varTag == "mixed")
    variance = Variance::Mixed;
  else {
    parser.emitError(parser.getNameLoc(), "unknown variance: ") << varTag;
    return Type();
  }

  if (failed(parser.parseGreater()))
    return Type();

  return FieldType::get(getContext(), elementType, rank, variance);
}

void TensoriumDialect::printType(Type type, DialectAsmPrinter &printer) const {
  TypeSwitch<Type>(type)
      .Case<FieldType>([&](FieldType t) {
        printer << "field<";
        printer.printType(t.getElementType());
        printer << ", " << t.getRank() << ", ";

        switch (t.getVariance()) {
        case Variance::Scalar:
          printer << "scalar";
          break;
        case Variance::Covariant:
          printer << "cov";
          break;
        case Variance::Contravariant:
          printer << "con";
          break;
        case Variance::Mixed:
          printer << "mixed";
          break;
        }

        printer << ">";
      })
      .Default([&](Type) { llvm_unreachable("unexpected 'tensorium' type"); });
}

} // namespace mlir
} // namespace tensorium


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/IR/TensoriumTypes.cpp
================================================================================
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumTypes.h"

using namespace tensorium::mlir;

FieldType FieldType::get(::mlir::MLIRContext *ctx, ::mlir::Type elementType,
                         unsigned rank, Variance variance) {
  return Base::get(ctx, elementType, rank, variance);
}

::mlir::Type FieldType::getElementType() const {
  return getImpl()->elementType;
}

unsigned FieldType::getRank() const { return getImpl()->rank; }

Variance FieldType::getVariance() const { return getImpl()->variance; }


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/Transforms/EinsteinLoweringPass.cpp
================================================================================
#include "tensorium_mlir/Dialect/Tensorium/Transform/EinsteinLoweringPass.h"

#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.h"

#include "mlir/IR/BuiltinAttributes.h"
#include "mlir/IR/PatternMatch.h"
#include "mlir/Pass/Pass.h"
#include "mlir/Transforms/GreedyPatternRewriteDriver.h"

#include "mlir/IR/Attributes.h"
#include "mlir/IR/Operation.h"
#include "mlir/IR/PatternMatch.h"
#include "mlir/IR/Value.h"
#include "mlir/Support/LLVM.h"
#include "llvm/ADT/SmallVector.h"
#include "llvm/Support/raw_ostream.h"

using namespace mlir;
static ArrayAttr getIndicesAttr(::mlir::Operation *op) {
  return op->getAttrOfType<ArrayAttr>("indices");
}

static bool
collectTensorRefsAndScalars(mlir::Value v,
                            llvm::SmallVector<mlir::Operation *, 8> &tensorRefs,
                            llvm::SmallVector<mlir::Value, 8> &scalars) {

  if (!v)
    return false;

  if (auto ref = v.getDefiningOp<tensorium::mlir::RefOp>()) {
    auto idx = getIndicesAttr(ref.getOperation());
    if (idx) {
      tensorRefs.push_back(ref.getOperation());
    } else {
      scalars.push_back(ref.getResult());
    }
    return true;
  }

  if (auto cst = v.getDefiningOp<tensorium::mlir::ConstOp>()) {
    scalars.push_back(cst.getResult());
    return true;
  }

  if (auto ctr = v.getDefiningOp<tensorium::mlir::ContractOp>()) {
    return collectTensorRefsAndScalars(ctr.getIn(), tensorRefs, scalars);
  }

  if (auto mul = v.getDefiningOp<tensorium::mlir::MulOp>()) {
    return collectTensorRefsAndScalars(mul.getLhs(), tensorRefs, scalars) &&
           collectTensorRefsAndScalars(mul.getRhs(), tensorRefs, scalars);
  }

  return false;
}
namespace tensorium::mlir {
namespace {

static llvm::SmallVector<std::string, 4> arrayAttrToStrings(ArrayAttr a) {
  llvm::SmallVector<std::string, 4> out;
  if (!a)
    return out;
  out.reserve(a.size());
  for (auto attr : a) {
    auto s = dyn_cast<StringAttr>(attr);
    if (!s)
      return {};
    out.push_back(s.getValue().str());
  }
  return out;
}

struct LowerContractToEinsum final
    : OpRewritePattern<tensorium::mlir::DtAssignOp> {
  using OpRewritePattern::OpRewritePattern;

  LogicalResult matchAndRewrite(tensorium::mlir::DtAssignOp op,
                                PatternRewriter &rewriter) const override {
    auto loc = op.getLoc();
    Value destField = op.getField();
    Value rhs = op.getRhs();

    auto lhsIdxAttr = cast<ArrayAttr>(op->getAttr("indices"));

    llvm::SmallVector<Operation *, 8> tensorRefs;
    llvm::SmallVector<Value, 8> scalars;

    auto process = [&](Value expr) -> LogicalResult {
      tensorRefs.clear();
      scalars.clear();

      if (!collectTensorRefsAndScalars(expr, tensorRefs, scalars))
        return failure();

      if (tensorRefs.size() == 0)
        return failure();

      llvm::SmallVector<llvm::StringRef, 8> lhsIdx;
      for (auto a : lhsIdxAttr)
        lhsIdx.push_back(cast<StringAttr>(a).getValue());

      llvm::SmallVector<llvm::SmallVector<llvm::StringRef, 4>, 8> ins;
      for (auto *r : tensorRefs) {
        llvm::SmallVector<llvm::StringRef, 4> v;
        for (auto a : getIndicesAttr(r))
          v.push_back(cast<StringAttr>(a).getValue());
        ins.push_back(v);
      }

      llvm::DenseMap<llvm::StringRef, unsigned> counts;
      for (auto &v : ins)
        for (auto s : v)
          counts[s]++;

      llvm::SmallVector<llvm::StringRef, 4> rhsFree;
      for (auto &it : counts)
        if (it.second == 1)
          rhsFree.push_back(it.first);

      if (rhsFree.size() != lhsIdx.size())
        return failure();

      for (auto s : rhsFree)
        if (!llvm::is_contained(lhsIdx, s))
          return failure();
      std::string spec;
      for (unsigned i = 0; i < ins.size(); ++i) {
        if (i)
          spec += ",";
        for (auto s : ins[i])
          spec += s.str();
      }
      spec += "->";
      for (auto s : lhsIdx)
        spec += s.str();

      llvm::SmallVector<Value, 8> inputs;
      for (auto *o : tensorRefs)
        inputs.push_back(o->getResult(0));

      auto specAttr =
          rewriter.getNamedAttr("spec", rewriter.getStringAttr(spec));

      auto eins = rewriter.create<tensorium::mlir::EinsumOp>(
          loc, rewriter.getF64Type(), inputs,
          llvm::ArrayRef<NamedAttribute>{specAttr});

      Value out = eins.getResult();
      for (Value s : scalars)
        out = rewriter.create<tensorium::mlir::MulOp>(loc, out, s).getResult();

      rewriter.create<tensorium::mlir::DtAssignOp>(loc, destField, out,
                                                   lhsIdxAttr);

      return success();
    };
    if (failed(process(rhs)))
      return failure();

    rewriter.eraseOp(op);
    return success();
  }
};

struct LowerContractOp final : OpRewritePattern<tensorium::mlir::ContractOp> {
  using OpRewritePattern::OpRewritePattern;

  static bool collectInsAndCounts(
      const llvm::SmallVector<Operation *, 8> &tensorRefs,
      llvm::SmallVector<llvm::SmallVector<llvm::StringRef, 4>, 8> &ins,
      llvm::DenseMap<llvm::StringRef, unsigned> &counts) {

    ins.clear();
    counts.clear();
    ins.reserve(tensorRefs.size());

    for (auto *r : tensorRefs) {
      auto idxAttr = getIndicesAttr(r);
      if (!idxAttr)
        return false;

      llvm::SmallVector<llvm::StringRef, 4> v;
      for (auto a : idxAttr) {
        auto s = cast<StringAttr>(a).getValue();
        v.push_back(s);
        counts[s] += 1;
      }
      ins.push_back(std::move(v));
    }
    return true;
  }

  static void collectFreeIndicesInOrder(
      const llvm::SmallVector<llvm::SmallVector<llvm::StringRef, 4>, 8> &ins,
      const llvm::DenseMap<llvm::StringRef, unsigned> &counts,
      llvm::SmallVector<llvm::StringRef, 8> &outIdx) {

    outIdx.clear();
    llvm::DenseSet<llvm::StringRef> seen;
    for (auto &v : ins) {
      for (auto s : v) {
        if (counts.lookup(s) == 1 && !seen.contains(s)) {
          outIdx.push_back(s);
          seen.insert(s);
        }
      }
    }
  }

  LogicalResult matchAndRewrite(tensorium::mlir::ContractOp op,
                                PatternRewriter &rewriter) const override {
    llvm::SmallVector<Operation *, 8> tensorRefs;
    llvm::SmallVector<Value, 8> scalars;

    if (!collectTensorRefsAndScalars(op.getIn(), tensorRefs, scalars))
      return failure();

    if (tensorRefs.empty())
      return failure();

    llvm::SmallVector<llvm::SmallVector<llvm::StringRef, 4>, 8> ins;
    llvm::DenseMap<llvm::StringRef, unsigned> counts;
    if (!collectInsAndCounts(tensorRefs, ins, counts))
      return failure();

    for (auto &it : counts) {
      if (it.second > 2)
        return failure();
    }

    llvm::SmallVector<llvm::StringRef, 8> outIdx;
    collectFreeIndicesInOrder(ins, counts, outIdx);

    std::string spec;
    for (unsigned i = 0; i < ins.size(); ++i) {
      if (i)
        spec += ",";
      for (auto s : ins[i])
        spec += s.str();
    }
    spec += "->";
    for (auto s : outIdx)
      spec += s.str();

    llvm::SmallVector<Value, 8> inputs;
    inputs.reserve(tensorRefs.size());
    for (auto *o : tensorRefs)
      inputs.push_back(o->getResult(0));

    auto eins = rewriter.create<tensorium::mlir::EinsumOp>(
        op.getLoc(), rewriter.getF64Type(), inputs,
        rewriter.getNamedAttr("spec", rewriter.getStringAttr(spec)));

    Value out = eins.getResult();
    for (Value s : scalars)
      out = rewriter.create<tensorium::mlir::MulOp>(op.getLoc(), out, s)
                .getResult();

    rewriter.replaceOp(op, out);
    return success();
  }
};

struct TensoriumEinsteinLoweringPass final
    : PassWrapper<TensoriumEinsteinLoweringPass, OperationPass<ModuleOp>> {
  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TensoriumEinsteinLoweringPass)

  void getDependentDialects(DialectRegistry &registry) const override {
    registry.insert<tensorium::mlir::TensoriumDialect>();
  }

  void runOnOperation() override {
    ModuleOp m = getOperation();

    RewritePatternSet patterns(&getContext());
    patterns.add<LowerContractToEinsum>(&getContext());
    patterns.add<LowerContractOp>(&getContext());

    GreedyRewriteConfig cfg;
    cfg.useTopDownTraversal = true;

    if (failed(applyPatternsGreedily(m, std::move(patterns), cfg)))
      signalPassFailure();
  }
};

} // namespace

std::unique_ptr<::mlir::Pass> createTensoriumEinsteinLoweringPass() {
  return std::make_unique<TensoriumEinsteinLoweringPass>();
}

} // namespace tensorium::mlir


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/Transforms/TsmOptimization.cpp
================================================================================


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/Transforms/IndexAnalyzePass.cpp
================================================================================
#include "tensorium_mlir/Dialect/Tensorium/Transform/IndexAnalyzePass.h"

#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.h"

#include "mlir/IR/BuiltinAttributes.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/IR/Diagnostics.h"
#include "mlir/Pass/Pass.h"
#include "tensorium_mlir/Semantic/Einstein.h"

#include "mlir/IR/Attributes.h"
#include "mlir/IR/Operation.h"
#include "mlir/IR/PatternMatch.h"
#include "mlir/IR/Value.h"
#include "mlir/Support/LLVM.h"
#include "llvm/ADT/MapVector.h"
#include "llvm/ADT/SmallVector.h"
#include "llvm/ADT/StringRef.h"
#include "llvm/Support/raw_ostream.h"

using namespace mlir;

namespace tensorium::mlir {
namespace {

static bool isStringArray(ArrayAttr a) {
  if (!a)
    return false;
  for (Attribute x : a)
    if (!isa<StringAttr>(x))
      return false;
  return true;
}

static ::llvm::SmallVector<::llvm::StringRef, 8> toRefs(ArrayAttr a) {
  ::llvm::SmallVector<::llvm::StringRef, 8> out;
  if (!a)
    return out;
  out.reserve(a.size());
  for (Attribute x : a) {
    auto s = dyn_cast<StringAttr>(x);
    if (!s)
      return {};
    out.push_back(s.getValue());
  }
  return out;
}

static ArrayAttr fromRefs(::mlir::OpBuilder &b,
                          ::mlir::ArrayRef<::llvm::StringRef> xs) {
  ::llvm::SmallVector<Attribute, 8> attrs;
  attrs.reserve(xs.size());
  for (auto s : xs)
    attrs.push_back(b.getStringAttr(s));
  return b.getArrayAttr(attrs);
}

static ArrayAttr arrayOfArraysFromVec(
    ::mlir::OpBuilder &b,
    ::mlir::ArrayRef<::llvm::SmallVector<::llvm::StringRef, 8>> vv) {
  ::llvm::SmallVector<Attribute, 8> out;
  out.reserve(vv.size());
  for (auto &v : vv)
    out.push_back(fromRefs(b, v));
  return b.getArrayAttr(out);
}

static DictionaryAttr
makeCounts(::mlir::OpBuilder &b,
           const ::llvm::MapVector<::llvm::StringRef, int64_t> &counts) {
  ::llvm::SmallVector<NamedAttribute, 16> kv;
  kv.reserve(counts.size());
  for (auto &it : counts)
    kv.push_back(b.getNamedAttr(it.first, b.getI64IntegerAttr(it.second)));
  return DictionaryAttr::get(b.getContext(), kv);
}


static ::llvm::SmallVector<::llvm::StringRef, 8>
readLhsIndices(tensorium::mlir::DtAssignOp op) {
  auto lhsIdx = op->getAttrOfType<ArrayAttr>("indices");
  if (!lhsIdx || !isStringArray(lhsIdx))
    return {};
  return toRefs(lhsIdx);
}


static DictionaryAttr makeRolesDictFromSemantic(
    ::mlir::OpBuilder &b,
    const tensorium::semantic::EinsteinAnalysisResult &sem) {
  ::llvm::SmallVector<NamedAttribute, 16> kv;
  kv.reserve(sem.all.size());
  for (auto idx : sem.all) {
    auto it = sem.roles.find(idx);
    auto role = (it == sem.roles.end())
                    ? llvm::StringRef("invalid")
                    : tensorium::semantic::roleToString(it->second);
    kv.push_back(b.getNamedAttr(idx, b.getStringAttr(role)));
  }
  return DictionaryAttr::get(b.getContext(), kv);
}

static llvm::SmallVector<llvm::StringRef, 32> collectIndices(Value v) {
  if (!v)
    return {};

  if (auto ref = v.getDefiningOp<tensorium::mlir::RefOp>()) {
    auto idx = ref->getAttrOfType<ArrayAttr>("indices");
    if (!idx)
      return {};
    llvm::SmallVector<llvm::StringRef, 8> out;
    for (auto a : idx)
      out.push_back(cast<StringAttr>(a).getValue());
    return out;
  }

  auto *def = v.getDefiningOp();
  if (!def)
    return {};

  auto name = def->getName().getStringRef();

  if (name == "tensorium.einsum") {
    llvm::SmallVector<llvm::StringRef, 32> out;
    for (auto in : def->getOperands()) {
      auto sub = collectIndices(in);
      out.append(sub.begin(), sub.end());
    }
    return out;
  }

  if (name == "tensorium.mul") { 
    auto a = collectIndices(def->getOperand(0));
    auto b = collectIndices(def->getOperand(1));
    a.append(b.begin(), b.end());
    return a;
  }

  if (name == "tensorium.add" ||
      name == "tensorium.sub") {
    return collectIndices(def->getOperand(0));
  }

  if (name == "tensorium.div") {
    return collectIndices(def->getOperand(0));
  }

  if (name == "tensorium.contract") {
    return collectIndices(def->getOperand(0));
  }

  if (name == "tensorium.deriv") {
    auto base = collectIndices(def->getOperand(0));
    auto idx = def->getAttrOfType<StringAttr>("index");
    if (idx)
      base.push_back(idx.getValue());
    return base;
  }

  return {};
}

struct TensoriumIndexAnalyzePass final
    : public ::mlir::PassWrapper<TensoriumIndexAnalyzePass,
                                 ::mlir::OperationPass<::mlir::ModuleOp>> {
  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TensoriumIndexAnalyzePass)

  void getDependentDialects(::mlir::DialectRegistry &registry) const override {
    registry.insert<tensorium::mlir::TensoriumDialect>();
  }

  void runOnOperation() override {
    ::mlir::ModuleOp m = this->getOperation();
    llvm::errs() << "[IndexAnalyze] ran\n";
    ::mlir::OpBuilder b(&this->getContext());

    m.walk([&](tensorium::mlir::DtAssignOp op) {
      b.setInsertionPoint(op);

      auto out = readLhsIndices(op);
      auto rhs = collectIndices(op->getOperand(1));

      tensorium::semantic::EinsteinAnalyzeOptions opt;
      opt.allowSummed = false;
      opt.allowDangling = false;

      auto sem = tensorium::semantic::analyzeEinstein(out, rhs, opt);

      ::llvm::SmallVector<::llvm::SmallVector<::llvm::StringRef, 8>, 4> ins =
          sem.ins;

      auto outAttr = fromRefs(b, sem.out);
      auto allAttr = fromRefs(b, sem.all);
      auto countsAttr = makeCounts(b, sem.counts);
      auto rolesAttr = makeRolesDictFromSemantic(b, sem);

      op->setAttr("tin.idx.ins", arrayOfArraysFromVec(b, ins));
      op->setAttr("tin.idx.out", outAttr);
      op->setAttr("tin.idx.all", allAttr);
      op->setAttr("tin.idx.counts", countsAttr);
      op->setAttr("tin.idx.roles", rolesAttr);
      op->setAttr("tin.idx.valid", b.getBoolAttr(sem.valid));

      if (!sem.valid) {
        op.emitError("invalid Einstein indices in dt_assign");
        return;
      }
    });
  }
};

} // namespace

std::unique_ptr<::mlir::Pass> createTensoriumIndexAnalyzePass() {
  return std::unique_ptr<::mlir::Pass>(new TensoriumIndexAnalyzePass());
}

} // namespace tensorium::mlir


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/Transforms/NoOpPass.cpp
================================================================================

#include "tensorium_mlir/Dialect/Tensorium/Transform/NoOpPass.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/Pass/Pass.h"

namespace tensorium {
namespace mlir {

namespace {

struct TensoriumNoOpPass
    : public ::mlir::PassWrapper<TensoriumNoOpPass,
                                 ::mlir::OperationPass<::mlir::ModuleOp>> {
  void runOnOperation() override {llvm::errs() << "[Tensorium] NoOp pass executed\n";}
};

} // namespace

std::unique_ptr<::mlir::Pass> createTensoriumNoOpPass() {
  return std::make_unique<TensoriumNoOpPass>();
}

} // namespace mlir
} // namespace tensorium



================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/Transforms/Passes.cpp
================================================================================
#include "tensorium_mlir/Dialect/Tensorium/Transform/Passes.h"
#include "mlir/Pass/Pass.h"

namespace tensorium {
namespace mlir {

void registerTensoriumTransformPasses() {
  ::mlir::registerPass([] { return createTensoriumNoOpPass(); });
  ::mlir::registerPass([] { return createTensoriumAnalysisPass(); });
}

} // namespace mlir
} // namespace tensorium


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/Transforms/IndexRoleAnalysisPass.cpp
================================================================================
#include "tensorium_mlir/Dialect/Tensorium/Transform/IndexRoleAnalysisPass.h"

#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.h"

#include "mlir/IR/BuiltinAttributes.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/Pass/Pass.h"
#include "llvm/ADT/DenseMap.h"
#include "llvm/ADT/SmallVector.h"

using namespace mlir;

namespace tensorium::mlir {
namespace {

static bool isAllStringAttrs(ArrayAttr a) {
  if (!a)
    return true;
  for (Attribute x : a)
    if (!isa<StringAttr>(x))
      return false;
  return true;
}

static llvm::SmallVector<StringRef, 8> toRefs(ArrayAttr a) {
  llvm::SmallVector<StringRef, 8> out;
  if (!a)
    return out;
  out.reserve(a.size());
  for (Attribute x : a)
    out.push_back(cast<StringAttr>(x).getValue());
  return out;
}

static bool contains(const llvm::SmallVector<StringRef, 8> &v, StringRef x) {
  for (auto s : v)
    if (s == x)
      return true;
  return false;
}

static void bumpCounts(const llvm::SmallVector<StringRef, 8> &idx,
                       llvm::DenseMap<StringRef, int64_t> &counts) {
  for (StringRef s : idx)
    counts[s] += 1;
}

static DictionaryAttr
makeCountsDict(MLIRContext &ctx,
               const llvm::DenseMap<StringRef, int64_t> &counts) {
  SmallVector<NamedAttribute, 16> attrs;
  attrs.reserve(counts.size());
  for (auto &kv : counts) {
    attrs.push_back(NamedAttribute(
        StringAttr::get(&ctx, kv.first),
        IntegerAttr::get(IntegerType::get(&ctx, 64), kv.second)));
  }
  return DictionaryAttr::get(&ctx, attrs);
}

static DictionaryAttr
makeRolesDict(MLIRContext &ctx,
              const llvm::DenseMap<StringRef, StringRef> &roles) {
  SmallVector<NamedAttribute, 16> attrs;
  attrs.reserve(roles.size());
  for (auto &kv : roles) {
    attrs.push_back(NamedAttribute(StringAttr::get(&ctx, kv.first),
                                   StringAttr::get(&ctx, kv.second)));
  }
  return DictionaryAttr::get(&ctx, attrs);
}

static ArrayAttr
makeAllIdxArray(MLIRContext &ctx,
                const llvm::DenseMap<StringRef, int64_t> &counts,
                const llvm::SmallVector<StringRef, 8> &outIdx) {
  llvm::DenseMap<StringRef, bool> seen;
  SmallVector<Attribute, 16> out;

  for (auto &kv : counts) {
    if (!seen[kv.first]) {
      seen[kv.first] = true;
      out.push_back(StringAttr::get(&ctx, kv.first));
    }
  }
  for (auto s : outIdx) {
    if (!seen[s]) {
      seen[s] = true;
      out.push_back(StringAttr::get(&ctx, s));
    }
  }

  return ArrayAttr::get(&ctx, out);
}

struct TensoriumIndexRoleAnalysisPass
    : public PassWrapper<TensoriumIndexRoleAnalysisPass,
                         OperationPass<ModuleOp>> {

  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TensoriumIndexRoleAnalysisPass)

  void getDependentDialects(DialectRegistry &registry) const override {
    registry.insert<tensorium::mlir::TensoriumDialect>();
  }

  void runOnOperation() override {
    ModuleOp m = getOperation();
    MLIRContext &ctx = getContext();

    m.walk([&](tensorium::mlir::DtAssignOp dt) {
      auto eins = dt.getRhs().getDefiningOp<tensorium::mlir::EinsumOp>();
      if (!eins)
        return;

      // === Récupération et validation du spec ===
      auto specAttr = eins->getAttrOfType<StringAttr>("spec");
      if (!specAttr) {
        eins->emitError("einsum missing spec attribute");
        signalPassFailure();
        return;
      }

      StringRef spec = specAttr.getValue();
      auto split = spec.split("->");
      StringRef outSpec = split.second; // peut être vide

      // === CAS SCALAIRE : spec = "...->" ===
      if (outSpec.empty()) {
        SmallVector<Attribute, 8> insLists;

        for (Value v : eins->getOperands()) {
          Operation *def = v.getDefiningOp();
          ArrayAttr inIdxAttr =
              def ? def->getAttrOfType<ArrayAttr>("indices") : ArrayAttr();
          if (!inIdxAttr)
            inIdxAttr = ArrayAttr::get(&ctx, {});
          insLists.push_back(inIdxAttr);
        }

        eins->setAttr("tin.idx.ins", ArrayAttr::get(&ctx, insLists));
        eins->setAttr("tin.idx.out", ArrayAttr::get(&ctx, {}));
        eins->setAttr("tin.idx.all", ArrayAttr::get(&ctx, {}));
        eins->setAttr("tin.idx.counts", DictionaryAttr::get(&ctx, {}));
        eins->setAttr("tin.idx.roles", DictionaryAttr::get(&ctx, {}));
        eins->setAttr("tin.idx.valid", BoolAttr::get(&ctx, true));
        return;
      }

      // === CAS TENSORIEL ===

      auto outIdxAttr = dt->getAttrOfType<ArrayAttr>("indices");
      if (!outIdxAttr)
        outIdxAttr = ArrayAttr::get(&ctx, {});
      if (!isAllStringAttrs(outIdxAttr)) {
        eins->emitError("dt_assign.indices must be ArrayAttr<StringAttr>");
        signalPassFailure();
        return;
      }
      auto outIdx = toRefs(outIdxAttr);

      SmallVector<Attribute, 8> insLists;
      llvm::DenseMap<StringRef, int64_t> counts;

      for (Value v : eins->getOperands()) {
        Operation *def = v.getDefiningOp();
        ArrayAttr inIdxAttr =
            def ? def->getAttrOfType<ArrayAttr>("indices") : ArrayAttr();
        if (!inIdxAttr)
          inIdxAttr = ArrayAttr::get(&ctx, {});
        if (!isAllStringAttrs(inIdxAttr)) {
          eins->emitError("einsum input indices must be ArrayAttr<StringAttr>");
          signalPassFailure();
          return;
        }
        insLists.push_back(inIdxAttr);
        bumpCounts(toRefs(inIdxAttr), counts);
      }

      llvm::DenseMap<StringRef, StringRef> roles;
      bool valid = true;

      auto all = makeAllIdxArray(ctx, counts, outIdx);
      for (Attribute a : all) {
        StringRef name = cast<StringAttr>(a).getValue();
        int64_t c = counts.lookup(name);
        bool inOut = contains(outIdx, name);

        if (inOut) {
          if (c == 1) {
            roles[name] = "free";
          } else {
            roles[name] = "invalid";
            valid = false;
          }
        } else {
          if (c >= 2) {
            roles[name] = "contracted";
          } else if (c == 1) {
            roles[name] = "dangling";
            valid = false;
          } else {
            roles[name] = "invalid";
            valid = false;
          }
        }
      }

      eins->setAttr("tin.idx.ins", ArrayAttr::get(&ctx, insLists));
      eins->setAttr("tin.idx.out", outIdxAttr);
      eins->setAttr("tin.idx.all", all);
      eins->setAttr("tin.idx.counts", makeCountsDict(ctx, counts));
      eins->setAttr("tin.idx.roles", makeRolesDict(ctx, roles));
      eins->setAttr("tin.idx.valid", BoolAttr::get(&ctx, valid));
    });
  }
};

} // namespace

std::unique_ptr<::mlir::Pass> createTensoriumIndexRoleAnalysisPass() {
  return std::make_unique<TensoriumIndexRoleAnalysisPass>();
}

} // namespace tensorium::mlir


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/Transforms/EinsteinAnalyzeEinsumPass.cpp
================================================================================

#include "tensorium_mlir/Dialect/Tensorium/Transform/EinsteinAnalyzeEinsumPass.h"

#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.h"
#include "tensorium_mlir/Semantic/Einstein.h"

#include "mlir/IR/BuiltinAttributes.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/IR/Diagnostics.h"
#include "mlir/IR/MLIRContext.h"
#include "mlir/Pass/Pass.h"

#include "llvm/ADT/SmallVector.h"
#include "llvm/ADT/StringRef.h"

using namespace mlir;

namespace tensorium::mlir {
namespace {

static bool isAllStringAttrs(ArrayAttr a) {
  if (!a)
    return true;
  for (Attribute x : a)
    if (!isa<StringAttr>(x))
      return false;
  return true;
}

static llvm::SmallVector<llvm::StringRef, 8> toRefs(ArrayAttr a) {
  llvm::SmallVector<llvm::StringRef, 8> out;
  if (!a)
    return out;
  out.reserve(a.size());
  for (Attribute x : a)
    out.push_back(cast<StringAttr>(x).getValue());
  return out;
}

static ArrayAttr fromRefs(OpBuilder &b, ArrayRef<llvm::StringRef> xs) {
  llvm::SmallVector<Attribute, 8> attrs;
  attrs.reserve(xs.size());
  for (auto s : xs)
    attrs.push_back(b.getStringAttr(s));
  return b.getArrayAttr(attrs);
}

static ArrayAttr fromRefs2D(
    OpBuilder &b,
    const llvm::SmallVector<llvm::SmallVector<llvm::StringRef, 8>, 4> &vv) {
  llvm::SmallVector<Attribute, 8> out;
  out.reserve(vv.size());
  for (auto &v : vv)
    out.push_back(fromRefs(b, v));
  return b.getArrayAttr(out);
}

static DictionaryAttr
makeCountsDict(OpBuilder &b,
               const llvm::MapVector<llvm::StringRef, int64_t> &counts) {
  llvm::SmallVector<NamedAttribute, 16> kv;
  kv.reserve(counts.size());
  for (auto &it : counts)
    kv.push_back(b.getNamedAttr(it.first, b.getI64IntegerAttr(it.second)));
  return DictionaryAttr::get(b.getContext(), kv);
}

static DictionaryAttr makeRolesDict(
    OpBuilder &b, ArrayRef<llvm::StringRef> all,
    const llvm::DenseMap<llvm::StringRef, tensorium::semantic::IndexRoleKind>
        &roles) {
  llvm::SmallVector<NamedAttribute, 16> kv;
  kv.reserve(all.size());
  for (auto idx : all) {
    auto it = roles.find(idx);
    llvm::StringRef r = (it == roles.end())
                            ? llvm::StringRef("invalid")
                            : tensorium::semantic::roleToString(it->second);
    kv.push_back(b.getNamedAttr(idx, b.getStringAttr(r)));
  }
  return DictionaryAttr::get(b.getContext(), kv);
}

static bool
parseSpecToIdx(MLIRContext *ctx, StringRef spec,
               llvm::SmallVector<llvm::SmallVector<llvm::StringRef, 8>, 4> &ins,
               llvm::SmallVector<llvm::StringRef, 8> &out) {
  auto parts = spec.split("->");
  if (parts.first.empty())
    return false;

  SmallVector<StringRef, 8> lhsTensors;
  parts.first.split(lhsTensors, ',', -1, false);

  ins.clear();
  out.clear();

  auto pushIndexChars = [&](StringRef s,
                            llvm::SmallVector<llvm::StringRef, 8> &dst) {
    s = s.trim();
    for (size_t i = 0; i < s.size(); ++i) {
      char ch = s[i];
      if (ch == ' ' || ch == '\t' || ch == '\n' || ch == '\r')
        continue;

      StringRef one(&s.data()[i], 1);
      dst.push_back(StringAttr::get(ctx, one).getValue());
    }
  };

  for (auto t : lhsTensors) {
    llvm::SmallVector<llvm::StringRef, 8> v;
    pushIndexChars(t, v);
    ins.push_back(std::move(v));
  }

  StringRef rhs = parts.second.trim();
  pushIndexChars(rhs, out);

  return true;
}

struct TensoriumEinsteinAnalyzeEinsumPass final
    : public PassWrapper<TensoriumEinsteinAnalyzeEinsumPass,
                         OperationPass<ModuleOp>> {

  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(
      TensoriumEinsteinAnalyzeEinsumPass)

  void getDependentDialects(DialectRegistry &registry) const override {
    registry.insert<tensorium::mlir::TensoriumDialect>();
  }

  void runOnOperation() override {
    ::mlir::ModuleOp mod = getOperation();
    ::mlir::MLIRContext &ctx = getContext();
    ::mlir::OpBuilder b(&ctx);

    tensorium::semantic::EinsteinAnalyzeOptions opt;
    opt.allowSummed = false;
    opt.allowDangling = false;

    bool failed = false;

    mod.walk([&](tensorium::mlir::EinsumOp op) {
      b.setInsertionPoint(op);

      llvm::SmallVector<llvm::SmallVector<llvm::StringRef, 8>, 4> ins;
      llvm::SmallVector<llvm::StringRef, 8> out;

      auto insAttr = op->getAttrOfType<::mlir::ArrayAttr>("tin.idx.ins");
      auto outAttr = op->getAttrOfType<::mlir::ArrayAttr>("tin.idx.out");

      if (insAttr) {
        for (::mlir::Attribute a : insAttr) {
          auto aa = ::mlir::dyn_cast<::mlir::ArrayAttr>(a);
          if (!aa || !isAllStringAttrs(aa)) {
            op.emitError(
                "tin.idx.ins must be ArrayAttr<ArrayAttr<StringAttr>>");
            failed = true;
            return;
          }
          ins.push_back(toRefs(aa));
        }
      }

      if (outAttr) {
        if (!isAllStringAttrs(outAttr)) {
          op.emitError("tin.idx.out must be ArrayAttr<StringAttr>");
          failed = true;
          return;
        }
        out = toRefs(outAttr);
      }

      if (ins.empty() || !outAttr) {
        auto specAttr = op->getAttrOfType<::mlir::StringAttr>("spec");
        if (!specAttr) {
          op.emitError("einsum missing 'spec' and missing tin.idx.ins/out");
          failed = true;
          return;
        }
        if (!parseSpecToIdx(&ctx, specAttr.getValue(), ins, out)) {
          op.emitError("failed to parse einsum spec");
          failed = true;
          return;
        }
      }

      if (ins.size() != op.getNumOperands()) {
        op.emitError("tin.idx.ins arity mismatch with einsum operands");
        failed = true;
        return;
      }

      llvm::SmallVector<llvm::StringRef, 32> rhsFlat;
      for (auto &v : ins)
        rhsFlat.append(v.begin(), v.end());

      auto sem = tensorium::semantic::analyzeEinstein(out, rhsFlat, opt);

      op->setAttr("tin.idx.ins", fromRefs2D(b, ins));
      op->setAttr("tin.idx.out", fromRefs(b, sem.out));
      op->setAttr("tin.idx.all", fromRefs(b, sem.all));
      op->setAttr("tin.idx.counts", makeCountsDict(b, sem.counts));
      op->setAttr("tin.idx.roles", makeRolesDict(b, sem.all, sem.roles));
      op->setAttr("tin.idx.valid", b.getBoolAttr(sem.valid));

      if (!sem.valid) {
        op.emitError("invalid Einstein indices on einsum");
        failed = true;
      }
    });

    if (failed)
      signalPassFailure();
  }
};
} // namespace

std::unique_ptr<::mlir::Pass> createTensoriumEinsteinAnalyzeEinsumPass() {
  return std::make_unique<TensoriumEinsteinAnalyzeEinsumPass>();
}

} // namespace tensorium::mlir


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/Transforms/AnalysisPass.cpp
================================================================================

#include "mlir/Dialect/Func/IR/FuncOps.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/IR/Operation.h"
#include "mlir/Pass/Pass.h"
#include "tensorium_mlir/Dialect/Tensorium/Transform/NoOpPass.h"
#include "llvm/Support/raw_ostream.h"

namespace tensorium {
namespace mlir {

namespace {

struct TensoriumAnalysisPass
    : public ::mlir::PassWrapper<TensoriumAnalysisPass,
                                 ::mlir::OperationPass<::mlir::ModuleOp>> {

  void runOnOperation() override {
    auto module = getOperation();

    size_t numFuncs = 0;
    size_t numBlocks = 0;
    size_t numOps = 0;

    module.walk([&](::mlir::Operation *) { ++numOps; });

    for (auto func : module.getOps<::mlir::func::FuncOp>()) {
      ++numFuncs;
      numBlocks += func.getBody().getBlocks().size();
    }

    llvm::errs() << "[Tensorium][Analysis] Module statistics\n";
    llvm::errs() << "  functions : " << numFuncs << "\n";
    llvm::errs() << "  blocks    : " << numBlocks << "\n";
    llvm::errs() << "  ops       : " << numOps << "\n";

    if (numFuncs == 0) {
      module.emitError("TensoriumAnalysisPass: module contains no functions");
      signalPassFailure();
    }
  }
};

} // namespace

std::unique_ptr<::mlir::Pass> createTensoriumAnalysisPass() {
  return std::make_unique<TensoriumAnalysisPass>();
}

} // namespace mlir
} // namespace tensorium


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/Transforms/EinsteinCanonicalizePass.cpp
================================================================================
#include "tensorium_mlir/Dialect/Tensorium/Transform/EinsteinCanonicalizePass.h"

#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.h"

#include "mlir/IR/BuiltinAttributes.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/IR/MLIRContext.h"
#include "mlir/Pass/Pass.h"

#include "llvm/ADT/DenseMap.h"
#include "llvm/ADT/MapVector.h"
#include "llvm/ADT/SmallVector.h"
#include "llvm/ADT/StringRef.h"

#include <algorithm>
#include <string>
#include <vector>

using namespace mlir;

namespace tensorium::mlir {
namespace {

static bool isStringArray(ArrayAttr a) {
  if (!a) return false;
  for (Attribute x : a)
    if (!isa<StringAttr>(x))
      return false;
  return true;
}

static llvm::SmallVector<StringRef, 8> toRefs(ArrayAttr a) {
  llvm::SmallVector<StringRef, 8> out;
  if (!a) return out;
  for (Attribute x : a)
    out.push_back(cast<StringAttr>(x).getValue());
  return out;
}

static ArrayAttr fromRefs(OpBuilder &b, ArrayRef<StringRef> xs) {
  llvm::SmallVector<Attribute, 8> attrs;
  for (auto s : xs)
    attrs.push_back(b.getStringAttr(s));
  return b.getArrayAttr(attrs);
}

static ArrayAttr fromRefs2D(OpBuilder &b,
                            ArrayRef<llvm::SmallVector<StringRef, 8>> vv) {
  llvm::SmallVector<Attribute, 8> out;
  for (auto &v : vv)
    out.push_back(fromRefs(b, v));
  return b.getArrayAttr(out);
}

static DictionaryAttr
makeCounts(OpBuilder &b, const llvm::MapVector<StringRef, int64_t> &counts) {
  llvm::SmallVector<NamedAttribute, 16> kv;
  for (auto &it : counts)
    kv.push_back(b.getNamedAttr(it.first, b.getI64IntegerAttr(it.second)));
  return DictionaryAttr::get(b.getContext(), kv);
}

static DictionaryAttr
makeRolesStrict(OpBuilder &b, ArrayRef<StringRef> all, ArrayRef<StringRef> out,
                const llvm::MapVector<StringRef, int64_t> &counts,
                bool &valid) {
  llvm::SmallDenseSet<StringRef, 16> outSet(out.begin(), out.end());
  llvm::SmallVector<NamedAttribute, 16> kv;
  valid = true;

  for (auto idx : all) {
    int64_t c = counts.lookup(idx);
    StringRef role = "invalid";

    if (outSet.contains(idx)) {
      role = "free";
      if (c != 1) valid = false;
    } else {
      role = (c == 2) ? "contracted" : "invalid";
      if (c != 2) valid = false;
    }

    kv.push_back(b.getNamedAttr(idx, b.getStringAttr(role)));
  }

  return DictionaryAttr::get(b.getContext(), kv);
}

static std::string joinNoSep(ArrayRef<StringRef> xs) {
  std::string s;
  for (auto x : xs) s += x.str();
  return s;
}

static std::string joinCommaNoSep(ArrayRef<std::string> xs) {
  std::string s;
  for (size_t i = 0; i < xs.size(); ++i) {
    if (i) s += ",";
    s += xs[i];
  }
  return s;
}

static llvm::SmallVector<StringRef, 16>
computeAllCanonical(ArrayRef<llvm::SmallVector<StringRef, 8>> ins,
                    ArrayRef<StringRef> outSorted) {
  llvm::SmallDenseSet<StringRef, 32> seen(outSorted.begin(), outSorted.end());
  llvm::SmallVector<StringRef, 16> rest;

  for (auto &v : ins)
    for (auto x : v)
      if (seen.insert(x).second)
        rest.push_back(x);

  llvm::sort(rest);
  llvm::SmallVector<StringRef, 16> all(outSorted.begin(), outSorted.end());
  all.append(rest.begin(), rest.end());
  return all;
}

static bool parseSpecToIdx(MLIRContext *ctx, StringRef spec,
                           llvm::SmallVector<llvm::SmallVector<StringRef, 8>, 8> &ins,
                           llvm::SmallVector<StringRef, 8> &out) {
  auto parts = spec.split("->");
  if (parts.second.empty()) return false;

  llvm::SmallVector<StringRef, 8> lhs;
  parts.first.split(lhs, ',', -1, false);

  ins.clear();
  out.clear();

  for (auto t : lhs) {
    llvm::SmallVector<StringRef, 8> v;
    for (char c : t.trim())
      v.push_back(StringAttr::get(ctx, StringRef(&c, 1)).getValue());
    ins.push_back(v);
  }

  for (char c : parts.second.trim())
    out.push_back(StringAttr::get(ctx, StringRef(&c, 1)).getValue());

  return true;
}

struct TensoriumEinsteinCanonicalizePass final
    : public PassWrapper<TensoriumEinsteinCanonicalizePass,
                         OperationPass<ModuleOp>> {

  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(
      TensoriumEinsteinCanonicalizePass)

  void getDependentDialects(DialectRegistry &registry) const override {
    registry.insert<tensorium::mlir::TensoriumDialect>();
  }

  void runOnOperation() override {
    ModuleOp m = getOperation();
    OpBuilder b(&getContext());

    m.walk([&](tensorium::mlir::EinsumOp op) {
      b.setInsertionPoint(op);

      llvm::SmallVector<llvm::SmallVector<StringRef, 8>, 8> ins;
      llvm::SmallVector<StringRef, 8> out;

      auto insAttr = op->getAttrOfType<ArrayAttr>("tin.idx.ins");
      auto outAttr = op->getAttrOfType<ArrayAttr>("tin.idx.out");

      if (insAttr && outAttr) {
        for (auto a : insAttr)
          ins.push_back(toRefs(cast<ArrayAttr>(a)));
        out = toRefs(outAttr);
      } else {
        auto specAttr = op->getAttrOfType<StringAttr>("spec");
        if (!specAttr || !parseSpecToIdx(&getContext(), specAttr.getValue(), ins, out))
          return;
      }

      if (ins.size() != op.getNumOperands()) return;

      struct Item { std::string key; unsigned pos; };
      std::vector<Item> order;
      for (unsigned i = 0; i < ins.size(); ++i)
        order.push_back({joinNoSep(ins[i]), i});

      std::stable_sort(order.begin(), order.end(),
        [](auto &a, auto &b){ return a.key < b.key; });

      llvm::SmallVector<Value, 8> newOps;
      llvm::SmallVector<llvm::SmallVector<StringRef, 8>, 8> newIns;
      std::vector<std::string> specIn;

      for (auto &it : order) {
        newOps.push_back(op.getOperand(it.pos));
        newIns.push_back(ins[it.pos]);
        specIn.push_back(joinNoSep(ins[it.pos]));
      }

      std::string spec = joinCommaNoSep(specIn) + "->" + joinNoSep(out);

      llvm::MapVector<StringRef, int64_t> counts;
      for (auto &v : newIns)
        for (auto x : v)
          counts[x]++;

      auto all = computeAllCanonical(newIns, out);
      bool valid = true;
      auto roles = makeRolesStrict(b, all, out, counts, valid);

      auto newOp = b.create<tensorium::mlir::EinsumOp>(
          op.getLoc(), op.getResult().getType(), newOps);

      newOp->setAttr("spec", b.getStringAttr(spec));
      newOp->setAttr("tin.idx.ins", fromRefs2D(b, newIns));
      newOp->setAttr("tin.idx.out", fromRefs(b, out));
      newOp->setAttr("tin.idx.all", fromRefs(b, all));
      newOp->setAttr("tin.idx.counts", makeCounts(b, counts));
      newOp->setAttr("tin.idx.roles", roles);
      newOp->setAttr("tin.idx.valid", b.getBoolAttr(valid));

      op.replaceAllUsesWith(newOp.getResult());
      op.erase();
    });
  }
};

} 

std::unique_ptr<::mlir::Pass> createTensoriumEinsteinCanonicalizePass() {
  return std::make_unique<TensoriumEinsteinCanonicalizePass>();
}

} 


================================================================================
FILE: lib/tensorium_mlir/Dialect/Tensorium/Transforms/EinsteinValidityPass.cpp
================================================================================
#include "tensorium_mlir/Dialect/Tensorium/Transform/EinsteinValidityPass.h"

#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.h"
#include "tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.h"

#include "mlir/IR/BuiltinAttributes.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/Pass/Pass.h"

using namespace mlir;

namespace tensorium::mlir {
namespace {

struct TensoriumEinsteinValidityPass final
    : public PassWrapper<TensoriumEinsteinValidityPass,
                         OperationPass<ModuleOp>> {

  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TensoriumEinsteinValidityPass)

  void getDependentDialects(DialectRegistry &registry) const override {
    registry.insert<tensorium::mlir::TensoriumDialect>();
  }

  void runOnOperation() override {
    ::mlir::ModuleOp mod = getOperation();

    bool failed = false;

    mod.walk([&](tensorium::mlir::EinsumOp op) {
      auto v = op->getAttrOfType<::mlir::BoolAttr>("tin.idx.valid");
      if (!v) {
        op.emitError("einsum missing tin.idx.valid (run "
                     "--tensorium-einstein-analyze-einsum)");
        failed = true;
        return;
      }
      if (!v.getValue()) {
        op.emitError("invalid Einstein indices on einsum");
        failed = true;
        return;
      }
    });

    mod.walk([&](tensorium::mlir::DtAssignOp op) {
      auto v = op->getAttrOfType<::mlir::BoolAttr>("tin.idx.valid");
      if (!v)
        return; // optional
      if (!v.getValue()) {
        op.emitError("invalid Einstein indices on dt_assign");
        failed = true;
      }
    });

    if (failed)
      signalPassFailure();
  }
};

} // namespace

std::unique_ptr<::mlir::Pass> createTensoriumEinsteinValidityPass() {
  return std::make_unique<TensoriumEinsteinValidityPass>();
}

} // namespace tensorium::mlir


================================================================================
FILE: lib/Lex/Lexer.cpp
================================================================================
#include "tensorium/Lex/Lexer.hpp"
#include <cctype>

namespace tensorium {
Lexer::Lexer(const char *input) : src(input) {}

void Lexer::advanceChar() {
  if (*src == '\n') {
    ++line;
    col = 1;
  } else {
    ++col;
  }
  ++src;
}

Token Lexer::next() {
  while (*src) {
    if (std::isspace((unsigned char)*src)) {
      advanceChar();
      continue;
    }
    if (*src == '#') {
      while (*src && *src != '\n')
        ++src;
      continue;
    }
    break;
  }
  if (!*src)
    return {TokenType::End, "", line, col};

  char c = *src;
  switch (c) {
  case '(':
    advanceChar();
    return {TokenType::LParen, "(", line, col - 1};
  case ')':
    advanceChar();
    return {TokenType::RParen, ")", line, col - 1};
  case '{':
    advanceChar();
    return {TokenType::LBrace, "{", line, col - 1};
  case '}':
    advanceChar();
    return {TokenType::RBrace, "}", line, col - 1};
  case '[':
    advanceChar();
    return {TokenType::LBracket, "[", line, col - 1};
  case ']':
    advanceChar();
    return {TokenType::RBracket, "]", line, col - 1};
  case ',':
    advanceChar();
    return {TokenType::Comma, ",", line, col - 1};
  case '=':
    advanceChar();
    return {TokenType::Equals, "=", line, col - 1};
  case '+':
    advanceChar();
    return {TokenType::Plus, "+", line, col - 1};
  case '-':
    advanceChar();
    return {TokenType::Minus, "-", line, col - 1};
  case '*':
    advanceChar();
    return {TokenType::Star, "*", line, col - 1};
  case '/':
    advanceChar();
    return {TokenType::Slash, "/", line, col - 1};
  case '^':
    advanceChar();
    return {TokenType::Caret, "^", line, col - 1};
  }

  if (isdigit((unsigned char)c) ||
      (c == '.' && isdigit((unsigned char)*(src + 1)))) {
    const char *s = src;
    while (isdigit((unsigned char)*src) || *src == '.')
      ++src;
    return {TokenType::Number, std::string(s, src), line, col};
  }

  if (isalpha((unsigned char)c)) {
    const char *s = src;
    while (isalnum((unsigned char)*src) || *src == '_')
      ++src;
    std::string t(s, src);
    if (t == "spacetime")
      return {TokenType::KwSpacetime, t, line, col};
    if (t == "metric")
      return {TokenType::KwMetric, t, line, col};
    if (t == "evolution")
      return {TokenType::KwEvolution, t, line, col};
    if (t == "dt")
      return {TokenType::KwDt, t, line, col};
    if (t == "field")
      return {TokenType::KwField, t, line, col};
    if (t == "scalar")
      return {TokenType::KwScalar, t, line, col};
    if (t == "vector")
      return {TokenType::KwVector, t, line, col};
    if (t == "covector")
      return {TokenType::KwCovector, t, line, col};
    if (t == "cov_tensor2")
      return {TokenType::KwCovTensor2, t, line, col};
    if (t == "con_tensor2")
      return {TokenType::KwConTensor2, t, line, col};
    if (t == "cov_tensor3")
      return {TokenType::KwCovTensor3, t, line, col};
    if (t == "con_tensor3")
      return {TokenType::KwConTensor3, t, line, col};
    if (t == "cov_tensor4")
      return {TokenType::KwCovTensor4, t, line, col};
    if (t == "con_tensor4")
      return {TokenType::KwConTensor4, t, line, col};
    if (t == "simulation")
      return {TokenType::KwSimulation, t, line, col};
    if (t == "time")
      return {TokenType::KwTime, t, line, col};
    if (t == "spatial")
      return {TokenType::KwSpatial, t, line, col};
    return {TokenType::Identifier, t, line, col};
  }
  std::string u(1, c);
  advanceChar();
  return {TokenType::Unknown, u, line, col - 1};
}
} // namespace tensorium


================================================================================
FILE: lib/Parse/Parser.cpp
================================================================================
#include "tensorium/Parse/Parser.hpp"
#include <stdexcept>

namespace tensorium {
Parser::Parser(Lexer &l) : lex(l) { advance(); }
void Parser::advance() { cur = lex.next(); }
void Parser::expect(TokenType type) {
  if (cur.type != type)
    syntaxError("Expected " + std::to_string((int)type));
  advance();
}
void Parser::syntaxError(const std::string &msg) {
  throw std::runtime_error("Syntax: " + msg + " at " +
                           std::to_string(cur.line));
}

std::unique_ptr<Expr> Parser::parseExpr() { return parseAddExpr(); }
std::unique_ptr<Expr> Parser::parseAddExpr() {
  auto left = parseMulExpr();
  while (cur.type == TokenType::Plus || cur.type == TokenType::Minus) {
    char op = cur.text[0];
    advance();
    left = std::make_unique<BinaryExpr>(std::move(left), op, parseMulExpr());
  }
  return left;
}
std::unique_ptr<Expr> Parser::parseMulExpr() {
  auto left = parsePowExpr();
  while (cur.type == TokenType::Star || cur.type == TokenType::Slash) {
    char op = cur.text[0];
    advance();
    left = std::make_unique<BinaryExpr>(std::move(left), op, parsePowExpr());
  }
  return left;
}
std::unique_ptr<Expr> Parser::parsePowExpr() {
  auto base = parseUnaryExpr();
  if (cur.type == TokenType::Caret) {
    advance();
    return std::make_unique<BinaryExpr>(std::move(base), '^', parsePowExpr());
  }
  return base;
}
std::unique_ptr<Expr> Parser::parseUnaryExpr() {
  if (cur.type == TokenType::Plus) {
    advance();
    return parseUnaryExpr();
  }
  if (cur.type == TokenType::Minus) {
    advance();
    return std::make_unique<BinaryExpr>(std::make_unique<NumberExpr>(0.0), '-',
                                        parseUnaryExpr());
  }
  return parsePrimary();
}
std::unique_ptr<Expr> Parser::parsePrimary() {
  if (cur.type == TokenType::Number) {
    double v = std::stod(cur.text);
    advance();
    return std::make_unique<NumberExpr>(v);
  }
  if (cur.type == TokenType::Identifier) {
    std::string n = cur.text;
    advance();
    if (cur.type == TokenType::LParen) {
      advance();
      auto args = parseExprList();
      expect(TokenType::RParen);
      auto c = std::make_unique<CallExpr>();
      c->callee = n;
      c->args = std::move(args);
      return c;
    }
    if (cur.type == TokenType::LBracket) {
      advance();
      std::vector<std::string> idx;
      while (cur.type == TokenType::Identifier) {
        idx.push_back(cur.text);
        advance();
        if (cur.type == TokenType::Comma) {
          advance();
          continue;
        }
        break;
      }
      expect(TokenType::RBracket);
      return std::make_unique<IndexedVarExpr>(n, std::move(idx));
    }
    return std::make_unique<VarExpr>(n);
  }
  if (cur.type == TokenType::LParen) {
    advance();
    auto e = parseExpr();
    expect(TokenType::RParen);
    return std::make_unique<ParenExpr>(std::move(e));
  }
  syntaxError("Unexpected token in expr");
}
std::vector<std::unique_ptr<Expr>> Parser::parseExprList() {
  std::vector<std::unique_ptr<Expr>> l;
  if (cur.type == TokenType::RParen)
    return l;
  l.push_back(parseExpr());
  while (cur.type == TokenType::Comma) {
    advance();
    l.push_back(parseExpr());
  }
  return l;
}

TensorAccess Parser::parseLHS() {
  TensorAccess lhs;
  if (cur.type != TokenType::Identifier)
    syntaxError("Expected ID on LHS");
  lhs.base = cur.text;
  advance();

  TokenType close = TokenType::Unknown;
  if (cur.type == TokenType::LBracket)
    close = TokenType::RBracket;
  else if (cur.type == TokenType::LParen)
    close = TokenType::RParen;

  if (close != TokenType::Unknown) {
    advance();
    while (cur.type == TokenType::Identifier) {
      lhs.indices.push_back(cur.text);
      advance();
      if (cur.type == TokenType::Comma) {
        advance();
        continue;
      }
      break;
    }
    expect(close);
  }
  return lhs;
}

Assignment Parser::parseAssignment() {
  Assignment a;
  a.lhs = parseLHS();
  expect(TokenType::Equals);
  a.rhs = parseExpr();
  return a;
}

FieldDecl Parser::parseFieldDecl() {
  expect(TokenType::KwField);

  TensorKind k;
  int u = 0, d = 0;

  if (cur.type == TokenType::KwScalar) {
    k = TensorKind::Scalar;
  } else if (cur.type == TokenType::KwVector) {
    k = TensorKind::Vector;
    u = 1;
  } else if (cur.type == TokenType::KwCovector) {
    k = TensorKind::Covector;
    d = 1;
  } else if (cur.type == TokenType::KwCovTensor2) {
    k = TensorKind::CovTensor2;
    d = 2;
  } else if (cur.type == TokenType::KwConTensor2) {
    k = TensorKind::ConTensor2;
    u = 2;
  } else if (cur.type == TokenType::KwCovTensor3) {
    k = TensorKind::CovTensor3;
    d = 3;
  } else if (cur.type == TokenType::KwConTensor3) {
    k = TensorKind::ConTensor3;
    u = 3;
  } else if (cur.type == TokenType::KwCovTensor4) {
    k = TensorKind::CovTensor4;
    d = 4;
  } else if (cur.type == TokenType::KwConTensor4) {
    k = TensorKind::ConTensor4;
    u = 4;
  } else {
    syntaxError("Unknown field type '" + cur.text + "'");
  }

  advance();

  if (cur.type != TokenType::Identifier)
    syntaxError("Expected field name");
  FieldDecl f;
  f.kind = k;
  f.up = u;
  f.down = d;
  f.name = cur.text;
  advance();
  if (cur.type == TokenType::LBracket) {
    advance();
    while (cur.type == TokenType::Identifier) {
      f.indices.push_back(cur.text);
      advance();
      if (cur.type == TokenType::Comma)
        advance();
      else
        break;
    }
    expect(TokenType::RBracket);
  }
  return f;
}

MetricDecl Parser::parseMetric() {
  expect(TokenType::KwMetric);
  if (cur.type != TokenType::Identifier)
    syntaxError("Metric name");
  MetricDecl m;
  m.name = cur.text;
  advance();
  expect(TokenType::LParen);
  while (cur.type == TokenType::Identifier) {
    m.indices.push_back(cur.text);
    advance();
    if (cur.type == TokenType::Comma)
      advance();
    else
      break;
  }
  expect(TokenType::RParen);
  expect(TokenType::LBrace);
  while (cur.type != TokenType::RBrace && cur.type != TokenType::End) {
    if (cur.type == TokenType::Identifier)
      m.entries.push_back(parseAssignment());
    else
      syntaxError("Unexpected in metric");
  }
  expect(TokenType::RBrace);
  return m;
}

EvolutionEq Parser::parseEvolutionEq() {
  expect(TokenType::KwDt);
  if (cur.type != TokenType::Identifier)
    syntaxError("Field name after dt");
  EvolutionEq eq;
  eq.fieldName = cur.text;
  advance();

  TokenType close = TokenType::Unknown;
  if (cur.type == TokenType::LBracket)
    close = TokenType::RBracket;
  else if (cur.type == TokenType::LParen)
    close = TokenType::RParen;

  if (close != TokenType::Unknown) {
    advance();
    while (cur.type == TokenType::Identifier) {
      eq.indices.push_back(cur.text);
      advance();
      if (cur.type == TokenType::Comma) {
        advance();
        continue;
      }
      break;
    }
    expect(close);
  }
  expect(TokenType::Equals);
  eq.rhs = parseExpr();
  return eq;
}

TimeConfig Parser::parseTimeBlock() {
  expect(TokenType::KwTime);
  expect(TokenType::LBrace);

  TimeConfig cfg;

  while (cur.type != TokenType::RBrace) {

    if (cur.text == "dt") {
      advance();
      expect(TokenType::Equals);
      if (cur.type != TokenType::Number)
        syntaxError("dt expects a number");
      cfg.dt = std::stod(cur.text);
      advance();
      continue;
    }

    if (cur.text == "integrator") {
      advance();
      expect(TokenType::Equals);

      if (cur.text == "euler")
        cfg.integrator = TimeIntegrator::Euler;
      else if (cur.text == "rk3")
        cfg.integrator = TimeIntegrator::RK3;
      else if (cur.text == "rk4")
        cfg.integrator = TimeIntegrator::RK4;
      else
        syntaxError("unknown time integrator");

      advance();
      continue;
    }

    syntaxError("unexpected entry in time block");
  }

  expect(TokenType::RBrace);
  return cfg;
}

SpatialConfig Parser::parseSpatialBlock() {
  expect(TokenType::KwSpatial);
  expect(TokenType::LBrace);

  SpatialConfig cfg;

  while (cur.type != TokenType::RBrace) {

    if (cur.text == "scheme") {
      advance();
      expect(TokenType::Equals);

      if (cur.text == "fd")
        cfg.scheme = SpatialScheme::FiniteDifference;
      else if (cur.text == "spectral")
        cfg.scheme = SpatialScheme::Spectral;
      else
        syntaxError("unknown spatial scheme");

      advance();
      continue;
    }

    if (cur.text == "derivative") {
      advance();
      expect(TokenType::Equals);

      if (cur.text == "centered")
        cfg.derivative = DerivativeScheme::Centered;
      else if (cur.text == "upwind")
        cfg.derivative = DerivativeScheme::Upwind;
      else
        syntaxError("unknown derivative scheme");

      advance();
      continue;
    }

    if (cur.text == "order") {
      advance();
      expect(TokenType::Equals);

      if (cur.type != TokenType::Number)
        syntaxError("order expects an integer");

      cfg.order = std::stoi(cur.text);
      advance();
      continue;
    }

    syntaxError("unexpected entry in spatial block");
  }

  expect(TokenType::RBrace);
  return cfg;
}

EvolutionDecl Parser::parseEvolution() {
  expect(TokenType::KwEvolution);
  if (cur.type != TokenType::Identifier)
    syntaxError("Evo name");
  EvolutionDecl evo;
  evo.name = cur.text;
  advance();
  expect(TokenType::LBrace);
  while (cur.type != TokenType::RBrace && cur.type != TokenType::End) {
    if (cur.type == TokenType::KwDt) {
      evo.equations.push_back(parseEvolutionEq());
      continue;
    }
    if (cur.type == TokenType::Identifier) {
      evo.tempAssignments.push_back(parseAssignment());
      continue;
    }
    syntaxError("Expected dt or assign");
  }
  expect(TokenType::RBrace);
  return evo;
}

SimulationConfig Parser::parseSimulation() {
  expect(TokenType::KwSimulation);
  expect(TokenType::LBrace);

  SimulationConfig cfg;

  while (cur.type != TokenType::RBrace) {

    if (cur.text == "coordinates") {
      advance();
      expect(TokenType::Equals);

      if (cur.text == "cartesian")
        cfg.coordinates = CoordinateSystem::Cartesian;
      else if (cur.text == "spherical")
        cfg.coordinates = CoordinateSystem::Spherical;
      else if (cur.text == "cylindrical")
        cfg.coordinates = CoordinateSystem::Cylindrical;
      else
        syntaxError("unknown coordinate system");

      advance();
      continue;
    }

    if (cur.text == "dimension") {
      advance();
      expect(TokenType::Equals);
      cfg.dimension = std::stoi(cur.text);
      expect(TokenType::Number);
      continue;
    }

    if (cur.text == "resolution") {
      advance();
      expect(TokenType::Equals);
      expect(TokenType::LBracket);

      cfg.resolution.clear();
      while (cur.type == TokenType::Number) {
        cfg.resolution.push_back(std::stoi(cur.text));
        advance();
        if (cur.type == TokenType::Comma)
          advance();
        else
          break;
      }

      expect(TokenType::RBracket);
      continue;
    }

    if (cur.type == TokenType::KwTime) {
      cfg.time = parseTimeBlock();
      continue;
    }

    if (cur.type == TokenType::KwSpatial) {
      cfg.spatial = parseSpatialBlock();
      continue;
    }

    syntaxError("unexpected entry in simulation block");
  }

  expect(TokenType::RBrace);
  return cfg;
}

Program Parser::parseProgram() {
  Program p;
  while (cur.type != TokenType::End) {
    if (cur.type == TokenType::KwField) {
      p.fields.push_back(parseFieldDecl());
      continue;
    }
    if (cur.type == TokenType::KwMetric) {
      p.metrics.push_back(parseMetric());
      continue;
    }
    if (cur.type == TokenType::KwEvolution) {
      p.evolutions.push_back(parseEvolution());
      continue;
    }
    if (cur.type == TokenType::KwSimulation) {
      if (p.simulation)
        syntaxError("Multiple simulation blocks not allowed");
      p.simulation = std::make_unique<SimulationConfig>(parseSimulation());
      continue;
    }
    syntaxError("Unexpected top level");
  }
  return p;
}
} // namespace tensorium


================================================================================
FILE: lib/Runtime/CpuRuntime.cpp
================================================================================

#include "tensorium/Runtime/CpuRuntime.hpp"
#include "tensorium/Runtime/Eval.hpp"
#include <stdexcept>

namespace tensorium::runtime {

static void require(bool cond, const std::string& msg) {
  if (!cond) throw std::runtime_error("runtime: " + msg);
}

CpuState1D initState1D(const backend::ModuleIR& mod,
                       double initScalar,
                       double initAlpha) {
  require(mod.simulation.has_value(), "simulation required");
  require(mod.simulation->dimension == 1, "only dimension=1 supported");

  CpuState1D st;
  st.n = static_cast<std::size_t>(mod.simulation->resolution.at(0));

  for (const auto& f : mod.fields) {
    require(f.up == 0 && f.down == 0, "only scalar fields supported (got " + f.name + ")");
    st.fields[f.name] = std::vector<double>(st.n, initScalar);
  }

  if (st.fields.count("alpha"))
    std::fill(st.fields["alpha"].begin(), st.fields["alpha"].end(), initAlpha);

  return st;
}

void runEuler1D(const backend::ModuleIR& mod, CpuState1D& st, const RunOptions& opt) {
  require(mod.simulation.has_value(), "simulation required");
  const double dt = mod.simulation->time.dt;

  require(mod.simulation->time.integrator == backend::TimeIntegrator::Euler,
          "only euler integrator supported");

  require(mod.evolutions.size() == 1, "runtime expects exactly 1 evolution block for now");

  const auto& evo = mod.evolutions[0];

  std::unordered_map<std::string, std::vector<double>> next = st.fields;

  for (std::size_t step = 0; step < opt.steps; ++step) {
    for (std::size_t i = 0; i < st.n; ++i) {
      ScalarEnv env;
      env.params = st.params;
      for (auto& kv : st.fields) {
        env.fieldPtr[kv.first] = &kv.second[i];
      }

      for (const auto& eq : evo.equations) {
        require(eq.indices.empty(),
                "tensor LHS not supported yet (field " + eq.fieldName + ")");

        auto it = st.fields.find(eq.fieldName);
        require(it != st.fields.end(), "unknown field on LHS: " + eq.fieldName);

        const double rhs = evalScalar(eq.rhs.get(), env);
        next[eq.fieldName][i] = it->second[i] + dt * rhs;
      }
    }

    st.fields.swap(next);
  }
}

} // namespace tensorium::runtime


================================================================================
FILE: lib/Runtime/Eval.cpp
================================================================================

#include "tensorium/Runtime/Eval.hpp"

namespace tensorium::runtime {

static double evalVar(const backend::VarIR* v, const ScalarEnv& env) {
  using backend::VarKind;

  if (v->vkind == VarKind::Field) {
    auto it = env.fieldPtr.find(v->name);
    if (it == env.fieldPtr.end() || !it->second)
      throw std::runtime_error("runtime: missing field value for '" + v->name + "'");
    return *(it->second);
  }

  if (v->vkind == VarKind::Param) {
    auto it = env.params.find(v->name);
    if (it == env.params.end())
      throw std::runtime_error("runtime: missing param '" + v->name + "'");
    return it->second;
  }

  throw std::runtime_error("runtime: unsupported var kind for '" + v->name + "'");
}

double evalScalar(const backend::ExprIR* e, const ScalarEnv& env) {
  using backend::ExprIR;

  if (!e) throw std::runtime_error("runtime: null expr");

  switch (e->kind) {
  case ExprIR::Kind::Number: {
    auto* n = static_cast<const backend::NumberIR*>(e);
    return n->value;
  }
  case ExprIR::Kind::Var: {
    auto* v = static_cast<const backend::VarIR*>(e);
    return evalVar(v, env);
  }
  case ExprIR::Kind::Binary: {
    auto* b = static_cast<const backend::BinaryIR*>(e);
    const double L = evalScalar(b->lhs.get(), env);
    const double R = evalScalar(b->rhs.get(), env);

    const std::string& op = b->op;
    if (op == "+") return L + R;
    if (op == "-") return L - R;
    if (op == "*") return L * R;
    if (op == "/") return L / R;

    throw std::runtime_error("runtime: unsupported binary op '" + op + "'");
  }
  case ExprIR::Kind::Call:
    throw std::runtime_error("runtime: calls not supported yet in scalar runtime");
  }

  throw std::runtime_error("runtime: unreachable");
}

} // namespace tensorium::runtime


================================================================================
FILE: lib/Sema/ProgramValidator.cpp
================================================================================
#include "tensorium/Sema/ProgramValidator.hpp"
#include <unordered_map>

using namespace tensorium;
using namespace tensorium::backend;
using namespace tensorium::sema;

static bool isValidIndexName(const std::string &s) {
  return s == "i" || s == "j" || s == "k" || s == "l";
}

static int fieldRank(const FieldIR &f) {
  return f.up + f.down;
}

ValidationResult sema::validateProgram(const ModuleIR &m) {
  ValidationResult res;

  if (!m.simulation.has_value()) {
    res.diags.push_back(
        {Diagnostic::Kind::Error, "missing simulation block"});
    return res;
  }

  const SimulationIR &sim = *m.simulation;

  if (sim.dimension <= 0) {
    res.diags.push_back(
        {Diagnostic::Kind::Error, "simulation dimension must be > 0"});
  }

  if ((int)sim.resolution.size() != sim.dimension) {
    res.diags.push_back(
        {Diagnostic::Kind::Error,
         "resolution size does not match simulation dimension"});
  }

  for (int r : sim.resolution) {
    if (r <= 0) {
      res.diags.push_back(
          {Diagnostic::Kind::Error, "resolution entries must be > 0"});
    }
  }

  std::unordered_map<std::string, const FieldIR *> fieldMap;
  for (auto &f : m.fields)
    fieldMap[f.name] = &f;

  for (auto &ev : m.evolutions) {
    for (auto &eq : ev.equations) {

      auto it = fieldMap.find(eq.fieldName);
      if (it == fieldMap.end()) {
        res.diags.push_back(
            {Diagnostic::Kind::Error,
             "unknown field in dt lhs: " + eq.fieldName});
        continue;
      }

      const FieldIR &f = *it->second;
      int rank = fieldRank(f);

      if ((int)eq.indices.size() != rank) {
        res.diags.push_back(
            {Diagnostic::Kind::Error,
             "wrong number of indices on lhs for field '" + f.name + "'"});
      }

      for (auto &idx : eq.indices) {
        if (!isValidIndexName(idx)) {
          res.diags.push_back(
              {Diagnostic::Kind::Error,
               "invalid index name: '" + idx + "'"});
        }
      }
    }
  }

  return res;
}


================================================================================
FILE: lib/Sema/Sema.cpp
================================================================================
#include "tensorium/Sema/Sema.hpp"
#include "tensorium/Sema/tensor_type_checker.hpp"
#include <algorithm>
#include <iostream>
#include <stdexcept>

namespace tensorium {

void SemanticAnalyzer::validateSpatialIndex(const std::string &idx) {
  if (!SPATIAL_INDICES.count(idx)) {
    throw std::runtime_error("Invalid tensor index '" + idx +
                             "'. Allowed: {i, j, k, l, m, n}.");
  }
}

int SemanticAnalyzer::resolveIndex(const std::string &name) {
  auto it = coordIndex.find(name);
  if (it == coordIndex.end())
    throw std::runtime_error("Unknown tensor index: " + name);
  return it->second;
}

std::unique_ptr<IndexedExpr> SemanticAnalyzer::transformExpr(const Expr *e) {
  if (auto n = dynamic_cast<const NumberExpr *>(e))
    return std::make_unique<IndexedNumber>(n->value);

  if (auto v = dynamic_cast<const VarExpr *>(e)) {
    if (auto it = coordIndex.find(v->name); it != coordIndex.end()) {
      auto iv =
          std::make_unique<IndexedVar>(v->name, IndexedVarKind::Coordinate);
      iv->coordIndex = it->second;
      iv->tensorKind = TensorKind::Scalar;
      return iv;
    }

    if (locals.count(v->name)) {
      auto iv = std::make_unique<IndexedVar>(v->name, IndexedVarKind::Local);
      iv->tensorKind = TensorKind::Scalar;
      return iv;
    }

    if (auto itf = fields.find(v->name); itf != fields.end()) {
      const FieldDecl *fd = itf->second;
      auto iv = std::make_unique<IndexedVar>(v->name, IndexedVarKind::Field);
      iv->tensorKind = fd->kind;
      iv->up = fd->up;
      iv->down = fd->down;
      return iv;
    }

    auto iv = std::make_unique<IndexedVar>(v->name, IndexedVarKind::Parameter);
    iv->tensorKind = TensorKind::Scalar;
    return iv;
  }

  if (auto b = dynamic_cast<const BinaryExpr *>(e))
    return std::make_unique<IndexedBinary>(b->op, transformExpr(b->lhs.get()),
                                           transformExpr(b->rhs.get()));

  if (auto p = dynamic_cast<const ParenExpr *>(e))
    return transformExpr(p->inner.get());

  if (auto iv = dynamic_cast<const IndexedVarExpr *>(e)) {
    auto it = fields.find(iv->base);
    if (it == fields.end())
      throw std::runtime_error("Unknown indexed tensor: " + iv->base);

    const FieldDecl *fd = it->second;
    size_t expected = static_cast<size_t>(fd->up + fd->down);

    if (iv->indices.size() != expected)
      throw std::runtime_error("Tensor '" + iv->base + "' expects " +
                               std::to_string(expected) + " indices, got " +
                               std::to_string(iv->indices.size()));

    auto out = std::make_unique<IndexedVar>(iv->base, IndexedVarKind::Field);
    out->tensorKind = fd->kind;
    out->up = fd->up;
    out->down = fd->down;

    for (auto &idx : iv->indices) {
      if (!coordIndex.count(idx)) {
        validateSpatialIndex(idx);
        coordIndex[idx] = -2;
      }
      int off = resolveIndex(idx);
      out->tensorIndices.push_back(off);
      out->tensorIndexNames.push_back(idx);
    }
    return out;
  }

  if (auto c = dynamic_cast<const CallExpr *>(e)) {
    auto out = std::make_unique<IndexedCall>();
    out->callee = c->callee;
    for (auto &arg : c->args)
      out->args.push_back(transformExpr(arg.get()));
    return out;
  }

  throw std::runtime_error("Unsupported expr in semantic analyzer");
}

SemanticAnalyzer::SemanticAnalyzer(const Program &p) : prog(p) {
  for (const auto &f : prog.fields) {
    if (fields.count(f.name))
      throw std::runtime_error("Field redeclared: " + f.name);
    fields[f.name] = &f;
  }

  for (const auto &m : prog.metrics) {
    for (const auto &entry : m.entries) {
      if (entry.lhs.indices.empty())
        locals[entry.lhs.base] = true;
    }
  }
  for (const auto &m : prog.metrics) {
    FieldDecl fd;
    fd.kind = TensorKind::CovTensor2;
    fd.name = m.name;
    fd.up = 0;
    fd.down = 2;
    syntheticMetricFields.push_back(fd);
    fields[m.name] = &syntheticMetricFields.back();
  }
  if (prog.simulation) {
    validateSimulation(*prog.simulation);
  }
}

IndexedMetric SemanticAnalyzer::analyzeMetric(const MetricDecl &decl) {
  coordIndex.clear();

  IndexedMetric out;
  out.name = decl.name;
  out.rank = 2;
  out.coords = decl.indices;

  for (size_t i = 0; i < decl.indices.size(); ++i)
    coordIndex[decl.indices[i]] = static_cast<int>(i);

  TensorTypeChecker checker;

  for (const auto &entry : decl.entries) {
    IndexedAssignment a;
    a.tensor = entry.lhs.base;

    if (!entry.lhs.indices.empty()) {
      if (entry.lhs.indices.size() != 2)
        throw std::runtime_error(
            "Metric tensor '" + entry.lhs.base + "' must have 2 indices (got " +
            std::to_string(entry.lhs.indices.size()) + ")");
      for (const auto &idx : entry.lhs.indices)
        a.indexOffsets.push_back(resolveIndex(idx));
    }

    a.rhs = transformExpr(entry.rhs.get());
    checker.checkMetricAssignment(a);
    out.assignments.push_back(std::move(a));
  }

  return out;
}

struct IndexCollector {
  std::unordered_map<std::string, int> &counter;
  IndexCollector(std::unordered_map<std::string, int> &c) : counter(c) {}

  static bool isSpatialIdx(const std::string &s) {
    return s == "i" || s == "j" || s == "k" || s == "l" || s == "m" || s == "n";
  }

  static bool isPartialDerivName(const std::string &s) {
    return s.size() == 3 && s[0] == 'd' && s[1] == '_' &&
           isSpatialIdx(std::string(1, s[2]));
  }

  static bool isNablaName(const std::string &s, char &idx) {
    if (s.size() == 7 && s.rfind("nabla_", 0) == 0) {
      idx = s[6];
      return isSpatialIdx(std::string(1, idx));
    }
    if (s.size() == 7 && s.rfind("nabla^", 0) == 0) {
      idx = s[6];
      return isSpatialIdx(std::string(1, idx));
    }
    return false;
  }

  void bumpLocalCounts(const IndexedExpr *expr,
                       std::unordered_map<std::string, int> &local) {
    if (!expr)
      return;

    if (auto v = dynamic_cast<const IndexedVar *>(expr)) {
      for (auto &idx : v->tensorIndexNames)
        if (!idx.empty() && isSpatialIdx(idx))
          local[idx] += 1;
      return;
    }

    if (auto b = dynamic_cast<const IndexedBinary *>(expr)) {
      bumpLocalCounts(b->lhs.get(), local);
      bumpLocalCounts(b->rhs.get(), local);
      return;
    }

    if (auto c = dynamic_cast<const IndexedCall *>(expr)) {
      for (auto &arg : c->args)
        bumpLocalCounts(arg.get(), local);

      const std::string &cal = c->callee;

      if (isPartialDerivName(cal)) {
        std::string idx(1, cal[2]);
        local[idx] += 1;
        return;
      }

      char nidx = 0;
      if (isNablaName(cal, nidx)) {
        std::string idx(1, nidx);
        local[idx] += 1;
        return;
      }

      return;
    }
  }

  void walk(const IndexedExpr *expr) {
    if (!expr)
      return;

    if (auto v = dynamic_cast<const IndexedVar *>(expr)) {
      for (auto &idx : v->tensorIndexNames)
        if (!idx.empty() && isSpatialIdx(idx))
          counter[idx] += 1;
      return;
    }

    if (auto b = dynamic_cast<const IndexedBinary *>(expr)) {
      walk(b->lhs.get());
      walk(b->rhs.get());
      return;
    }

    if (auto c = dynamic_cast<const IndexedCall *>(expr)) {
      if (c->callee == "contract") {
        if (c->args.size() != 1)
          throw std::runtime_error("contract() expects 1 argument");

        std::unordered_map<std::string, int> local;
        bumpLocalCounts(c->args[0].get(), local);

        for (auto &kv : local) {
          const std::string &idx = kv.first;
          int cnt = kv.second;
          if (cnt == 1) {
            counter[idx] += 1;
          } else if (cnt == 2) {
          } else if (cnt > 2) {
            throw std::runtime_error("Ambiguous contraction: index '" + idx +
                                     "' appears " + std::to_string(cnt) +
                                     " times.");
          }
        }
        return;
      }

      for (auto &arg : c->args)
        walk(arg.get());

      const std::string &cal = c->callee;

      if (isPartialDerivName(cal)) {
        std::string idx(1, cal[2]);
        counter[idx] += 1;
        return;
      }

      char nidx = 0;
      if (isNablaName(cal, nidx)) {
        std::string idx(1, nidx);
        counter[idx] += 1;
        return;
      }

      return;
    }
  }
};

IndexedEvolution SemanticAnalyzer::analyzeEvolution(const EvolutionDecl &evo) {
  coordIndex.clear();

  IndexedEvolution out;
  out.name = evo.name;

  for (const auto &eq : evo.equations)
    for (const auto &idx : eq.indices) {
      validateSpatialIndex(idx);
      coordIndex[idx] = -1;
    }

  for (const auto &tmp : evo.tempAssignments) {
    if (!tmp.lhs.indices.empty()) {
      continue;
    }

    if (fields.count(tmp.lhs.base)) {
      throw std::runtime_error("Cannot redeclare field '" + tmp.lhs.base +
                               "' as local");
    }

    locals[tmp.lhs.base] = true;
  }

  TensorTypeChecker checker;

  for (const auto &eq : evo.equations) {

    auto it = fields.find(eq.fieldName);
    if (it == fields.end())
      throw std::runtime_error("Unknown field in evolution: " + eq.fieldName);

    const FieldDecl *fd = it->second;
    size_t expectedRank = static_cast<size_t>(fd->up + fd->down);

    if (eq.indices.size() != expectedRank) {
      throw std::runtime_error(
          "Wrong number of indices in evolution for field '" + eq.fieldName +
          "': expected " + std::to_string(expectedRank) + ", got " +
          std::to_string(eq.indices.size()));
    }

    indexUseCount.clear();
    lhsIndices.clear();

    for (auto &idx : eq.indices)
      lhsIndices.insert(idx);

    IndexedEvolutionEq ie;
    ie.fieldName = eq.fieldName;
    ie.indices = eq.indices;
    ie.rhs = transformExpr(eq.rhs.get());

    IndexCollector collector(indexUseCount);
    collector.walk(ie.rhs.get());

    for (auto &[idx, count] : indexUseCount) {
      validateSpatialIndex(idx);

      if (count == 1 && !lhsIndices.count(idx)) {
        throw std::runtime_error("Free index '" + idx +
                                 "' appears only in RHS and not LHS.");
      }

      if (count > 2 && !lhsIndices.count(idx)) {
        throw std::runtime_error("Ambiguous contraction: index '" + idx +
                                 "' appears " + std::to_string(count) +
                                 " times.");
      }
    }

    TensorType lhsType = {fd->up, fd->down};
    checker.checkAssignmentVariance(lhsType, ie.indices, ie.rhs.get());

    out.equations.push_back(std::move(ie));
  }

  for (const auto &tmp : evo.tempAssignments) {
    IndexedAssignment ia;
    ia.tensor = tmp.lhs.base;
    for (auto &idx : tmp.lhs.indices)
      ia.indexOffsets.push_back(resolveIndex(idx));
    ia.rhs = transformExpr(tmp.rhs.get());
    out.temp.push_back(std::move(ia));
  }

  return out;
}

void SemanticAnalyzer::validateSimulation(const SimulationConfig &sim) {
  if (sim.dimension <= 0) {
    throw std::runtime_error("simulation dimension must be >= 1");
  }

  if ((int)sim.resolution.size() != sim.dimension) {
    throw std::runtime_error(
        "resolution size (" + std::to_string(sim.resolution.size()) +
        ") does not match dimension (" + std::to_string(sim.dimension) + ")");
  }

  for (int r : sim.resolution) {
    if (r <= 0)
      throw std::runtime_error("resolution entries must be > 0");
  }

  if (sim.time.dt <= 0.0) {
    throw std::runtime_error("time.dt must be > 0");
  }

  if (sim.spatial.scheme == SpatialScheme::FiniteDifference) {
    if (sim.spatial.order < 2)
      throw std::runtime_error("FD order must be >= 2");

    if (sim.spatial.order % 2 != 0)
      throw std::runtime_error("FD order must be even");
  }

  if (sim.spatial.scheme == SpatialScheme::Spectral) {
    if (sim.spatial.order != 0) {
      throw std::runtime_error(
          "spectral scheme does not use finite-difference order");
    }
  }
}
} // namespace tensorium


================================================================================
FILE: lib/Backend/BackendBuilder.cpp
================================================================================

#include "tensorium/Backend/BackendBuilder.hpp"

namespace tensorium::backend {

static std::unique_ptr<ExprIR>
lowerIndexedExpr(const tensorium::IndexedExpr *e) {
  using namespace tensorium;

  if (!e)
    return nullptr;

  if (auto n = dynamic_cast<const IndexedNumber *>(e)) {
    return std::make_unique<NumberIR>(n->value);
  }

  if (auto v = dynamic_cast<const IndexedVar *>(e)) {
    VarKind k = VarKind::Field;
    int coord = -1;
    switch (v->kind) {
    case IndexedVarKind::Field:
      k = VarKind::Field;
      break;
    case IndexedVarKind::Parameter:
      k = VarKind::Param;
      break;
    case IndexedVarKind::Local:
      k = VarKind::Local;
      break;
    case IndexedVarKind::Coordinate:
      k = VarKind::Coord;
      coord = v->coordIndex;
      break;
    }

    auto out = std::make_unique<VarIR>(v->name, k);
    out->coordIndex = coord;
    out->tensorIndexNames = v->tensorIndexNames;
    return out;
  }

  if (auto b = dynamic_cast<const IndexedBinary *>(e)) {

    return std::make_unique<BinaryIR>(std::string(1, b->op),
                                      lowerIndexedExpr(b->lhs.get()),
                                      lowerIndexedExpr(b->rhs.get()));
  }

  if (auto c = dynamic_cast<const IndexedCall *>(e)) {
    auto out = std::make_unique<CallIR>(c->callee);
    out->args.reserve(c->args.size());
    for (const auto &a : c->args)
      out->args.push_back(lowerIndexedExpr(a.get()));
    return out;
  }

  return std::make_unique<CallIR>("<unknown>");
}

FieldKind BackendBuilder::lowerFieldKind(TensorKind k) {
  switch (k) {
  case TensorKind::Scalar:
    return FieldKind::Scalar;
  case TensorKind::Vector:
    return FieldKind::Vector;
  case TensorKind::Covector:
    return FieldKind::Covector;
  case TensorKind::CovTensor2:
    return FieldKind::CovTensor2;
  case TensorKind::ConTensor2:
    return FieldKind::ConTensor2;
  case TensorKind::CovTensor3:
    return FieldKind::CovTensor3;
  case TensorKind::ConTensor3:
    return FieldKind::ConTensor4;
  case TensorKind::CovTensor4:
    return FieldKind::CovTensor4;
  case TensorKind::ConTensor4:
    return FieldKind::ConTensor4;
  case TensorKind::MixedTensor:
    return FieldKind::MixedTensor;
  }
  return FieldKind::Scalar;
}

static SimulationIR lowerSimulation(const tensorium::SimulationConfig &sim) {
  SimulationIR out;

  switch (sim.coordinates) {
  case tensorium::CoordinateSystem::Cartesian:
    out.coords = CoordSystem::Cartesian;
    break;
  case tensorium::CoordinateSystem::Spherical:
    out.coords = CoordSystem::Spherical;
    break;
  case tensorium::CoordinateSystem::Cylindrical:
    out.coords = CoordSystem::Cylindrical;
    break;
  }

  out.dimension = sim.dimension;
  out.resolution = sim.resolution;

  // Time
  out.time.dt = sim.time.dt;
  switch (sim.time.integrator) {
  case tensorium::TimeIntegrator::Euler:
    out.time.integrator = backend::TimeIntegrator::Euler;
    break;
  case tensorium::TimeIntegrator::RK3:
    out.time.integrator = backend::TimeIntegrator::RK3;
    break;
  case tensorium::TimeIntegrator::RK4:
    out.time.integrator = backend::TimeIntegrator::RK4;
    break;
  }

  // Spatial
  out.spatial.order = sim.spatial.order;

  out.spatial.scheme =
      (sim.spatial.scheme == tensorium::SpatialScheme::FiniteDifference)
          ? backend::SpatialScheme::FD
          : backend::SpatialScheme::Spectral;

  out.spatial.derivative =
      (sim.spatial.derivative == tensorium::DerivativeScheme::Centered)
          ? backend::DerivativeScheme::Centered
          : backend::DerivativeScheme::Upwind;

  return out;
}

ModuleIR BackendBuilder::build(const Program &prog,
                               tensorium::SemanticAnalyzer &sem) {
  ModuleIR mod;

  if (prog.simulation)
    mod.simulation = lowerSimulation(*prog.simulation);

  mod.fields.reserve(prog.fields.size());
  for (const auto &f : prog.fields) {
    FieldIR out;
    out.name = f.name;
    out.kind = lowerFieldKind(f.kind);
    out.up = f.up;
    out.down = f.down;
    mod.fields.push_back(std::move(out));
  }

  mod.evolutions.reserve(prog.evolutions.size());
  for (const auto &evo : prog.evolutions) {
    auto indexed = sem.analyzeEvolution(evo);

    EvolutionIR out;
    out.name = indexed.name;
    out.equations.reserve(indexed.equations.size());

    for (const auto &eq : indexed.equations) {
      EquationIR oeq;
      oeq.fieldName = eq.fieldName;
      oeq.indices = eq.indices;
      oeq.rhs = lowerIndexedExpr(eq.rhs.get());
      out.equations.push_back(std::move(oeq));
    }

    mod.evolutions.push_back(std::move(out));
  }

  return mod;
}

} // namespace tensorium::backend


================================================================================
FILE: lib/AST/Expr.cpp
================================================================================
#include "tensorium/AST/AST.hpp"

namespace tensorium {
//
// NumberExpr::NumberExpr(double v) : value(v) {}
// void NumberExpr::accept(ExprVisitor &V) const { V.visit(*this); }
//
// VarExpr::VarExpr(std::string n) : name(std::move(n)) {}
// void VarExpr::accept(ExprVisitor &V) const { V.visit(*this); }
//
// BinaryExpr::BinaryExpr(std::unique_ptr<Expr> L, char Op,
//                        std::unique_ptr<Expr> R)
//     : lhs(std::move(L)), rhs(std::move(R)), op(Op) {}
// void BinaryExpr::accept(ExprVisitor &V) const { V.visit(*this); }
//
// ParenExpr::ParenExpr(std::unique_ptr<Expr> e) : inner(std::move(e)) {}
// void ParenExpr::accept(ExprVisitor &V) const { V.visit(*this); }
//
// void CallExpr::accept(ExprVisitor &V) const { V.visit(*this); }
//
// IndexedVarExpr::IndexedVarExpr(std::string b, std::vector<std::string> idx)
//     : base(std::move(b)), indices(std::move(idx)) {}
// void IndexedVarExpr::accept(ExprVisitor &V) const { V.visit(*this); }
//
} // namespace tensorium


================================================================================
FILE: lib/AST/ASTPrinter.cpp
================================================================================
#include "tensorium/AST/ASTPrinter.hpp"
#include "tensorium/AST/AST.hpp"
#include "tensorium/AST/Visitor.hpp"
#include <iostream>

namespace tensorium {

class ASTPrinter : public ExprVisitor {
public:
  void visit(const NumberExpr &E) override { std::cout << E.value; }

  void visit(const VarExpr &E) override { std::cout << E.name; }

  void visit(const BinaryExpr &E) override {
    std::cout << "(";
    E.lhs->accept(*this);
    std::cout << " " << E.op << " ";
    E.rhs->accept(*this);
    std::cout << ")";
  }

  void visit(const ParenExpr &E) override {
    std::cout << "(";
    E.inner->accept(*this);
    std::cout << ")";
  }

  void visit(const CallExpr &E) override {
    std::cout << E.callee << "(";
    for (size_t i = 0; i < E.args.size(); ++i) {
      E.args[i]->accept(*this);
      if (i + 1 < E.args.size())
        std::cout << ", ";
    }
    std::cout << ")";
  }

  void visit(const IndexedVarExpr &E) override {
    std::cout << E.base << "[";
    for (size_t i = 0; i < E.indices.size(); ++i) {
      std::cout << E.indices[i];
      if (i + 1 < E.indices.size())
        std::cout << ",";
    }
    std::cout << "]";
  }
};

void printExpr(const Expr *e) {
  if (!e)
    return;
  ASTPrinter P;
  e->accept(P);
}

void printProgram(const Program &prog) {
  std::cout << "=== Program AST ===\n";
  if (!prog.fields.empty()) {
    std::cout << "\nFields:\n";
    for (const auto &f : prog.fields) {
      std::cout << "  field ";

      switch (f.kind) {
      case TensorKind::Scalar:
        std::cout << "scalar ";
        break;
      case TensorKind::Vector:
        std::cout << "vector ";
        break;
      case TensorKind::Covector:
        std::cout << "covector ";
        break;
      case TensorKind::CovTensor2:
        std::cout << "cov_tensor2";
        break;
      case TensorKind::ConTensor2:
        std::cout << "con_tensor2";
        break;
	  case TensorKind::CovTensor3:
        std::cout << "cov_tensor3";
        break;
      case TensorKind::ConTensor3:
        std::cout << "con_tensor3";
        break;
      case TensorKind::ConTensor4:
        std::cout << "con_tensor4";
        break;
      case TensorKind::CovTensor4:
        std::cout << "cov_tensor4";
        break;
      case TensorKind::MixedTensor:
        std::cout << "mixed_tensor";
        break;
      }

      std::cout << f.name;

      if (!f.indices.empty()) {
        std::cout << "[";
        for (size_t i = 0; i < f.indices.size(); ++i) {
          std::cout << f.indices[i];
          if (i + 1 < f.indices.size())
            std::cout << ",";
        }
        std::cout << "]";
      }
      std::cout << "\n";
    }
  }

  if (prog.simulation) {
    const auto &sim = *prog.simulation;

    std::cout << "\nSimulation:\n";
    std::cout << "  dimension = " << sim.dimension << "\n";

    std::cout << "  resolution = [";
    for (size_t i = 0; i < sim.resolution.size(); ++i) {
      std::cout << sim.resolution[i];
      if (i + 1 < sim.resolution.size())
        std::cout << ",";
    }
    std::cout << "]\n";

    std::cout << "  time:\n";
    std::cout << "    dt = " << sim.time.dt << "\n";

    std::cout << "  spatial:\n";
    std::cout << "    order = " << sim.spatial.order << "\n";
  }
  for (const auto &evo : prog.evolutions) {
    std::cout << "Evolution " << evo.name << " {\n";
    for (const auto &eq : evo.equations) {
      std::cout << "  dt " << eq.fieldName;
      if (!eq.indices.empty()) {
        std::cout << "[";
        for (size_t i = 0; i < eq.indices.size(); ++i) {
          std::cout << eq.indices[i];
          if (i + 1 < eq.indices.size())
            std::cout << ",";
        }
        std::cout << "]";
      }
      std::cout << " = ";
      printExpr(eq.rhs.get());
      std::cout << "\n";
    }
    std::cout << "}\n";
  }
}

} // namespace tensorium


================================================================================
FILE: tools/CMakeLists.txt
================================================================================

add_subdirectory(Tester)
add_subdirectory(driver)



================================================================================
FILE: tools/driver/CMakeLists.txt
================================================================================

add_executable(Tensorium_cc
  main.cpp
)

target_link_libraries(Tensorium_cc PRIVATE
  tensorium
  tensorium_mlir_backend
)



================================================================================
FILE: tools/driver/main.cpp
================================================================================
#include "tensorium/AST/ASTPrinter.hpp"
#include "tensorium/Lex/Lexer.hpp"
#include "tensorium/Parse/Parser.hpp"
#include "tensorium/Sema/Sema.hpp"

#include "tensorium/Backend/BackendBuilder.hpp"
#include "tensorium/Backend/DomainIR.hpp"
#include "tensorium/Backend/IRPrinter.hpp"
#include "tensorium/Runtime/CpuRuntime.hpp"
#include "tensorium/Runtime/Eval.hpp"
#include "tensorium/Sema/ProgramValidator.hpp"
#include "tensorium_mlir/Target/MLIRGen/MLIRGen.h"

#include <fstream>
#include <iostream>
#include <sstream>
#include <vector>

using namespace tensorium;

static std::string readFile(const std::string &path) {
  std::ifstream file(path);
  if (!file)
    throw std::runtime_error("cannot open file: " + path);

  std::ostringstream ss;
  ss << file.rdbuf();
  return ss.str();
}

static void printIndexedExpr(const IndexedExpr *e) {
  if (auto n = dynamic_cast<const IndexedNumber *>(e)) {
    std::cout << n->value;
    return;
  }

  if (auto v = dynamic_cast<const IndexedVar *>(e)) {
    std::cout << v->name << "[";

    switch (v->kind) {
    case IndexedVarKind::Field:
      std::cout << "field";
      break;
    case IndexedVarKind::Parameter:
      std::cout << "param";
      break;
    case IndexedVarKind::Local:
      std::cout << "local";
      break;
    case IndexedVarKind::Coordinate:
      std::cout << "coord:" << v->coordIndex;
      break;
    }

    if (!v->tensorIndexNames.empty()) {
      std::cout << ";";
      for (size_t i = 0; i < v->tensorIndexNames.size(); ++i) {
        std::cout << v->tensorIndexNames[i];
        if (i + 1 < v->tensorIndexNames.size())
          std::cout << ",";
      }
    }

    std::cout << "]";
    return;
  }

  if (auto b = dynamic_cast<const IndexedBinary *>(e)) {
    std::cout << "(";
    printIndexedExpr(b->lhs.get());
    std::cout << " " << b->op << " ";
    printIndexedExpr(b->rhs.get());
    std::cout << ")";
    return;
  }

  if (auto c = dynamic_cast<const IndexedCall *>(e)) {
    std::cout << c->callee << "(";
    for (size_t i = 0; i < c->args.size(); ++i) {
      printIndexedExpr(c->args[i].get());
      if (i + 1 < c->args.size())
        std::cout << ", ";
    }
    std::cout << ")";
    return;
  }
}

int main(int argc, char **argv) {
  bool dumpAST = false;
  bool dumpIndexed = false;
  bool dumpBackend, dumpBackendExpr = false;
  bool runCpu = false;
  size_t steps = 10;
  double initScalar = 1.0;
  double initAlpha = 2.0;
  bool dumpMLIR = false;
  bool enableNoOpPass = false;
  bool enableAnalysisPass = false;
  bool validateOnly = false;
  bool enableEinsteinLoweringPass = false;
  bool enableEinsteinValidityPass = false;
  bool enableIndexAnalyzePass = false;
  bool enableEinsteinCanonicalizePass = false;
  bool enableEinsteinAnalyzeEinsumPass = false;

  if (argc < 2) {
    std::cerr << "usage: Tensorium_cc [--dump-ast] file1.tn [file2.tn ...]\n";
    return 1;
  }

  std::vector<std::string> files;

  for (int i = 1; i < argc; ++i) {
    std::string arg = argv[i];

    if (arg == "--dump-ast") {
      dumpAST = true;
    } else if (arg == "--dump-indexed") {
      dumpIndexed = true;
    } else if (arg == "--dump-backend") {
      dumpBackend = true;
    } else if (arg == "--dump-backend-expr") {
      dumpBackendExpr = true;
    } else if (arg == "--tensorium-noop") {
      enableNoOpPass = true;
    } else if (arg == "--tensorium-analyze") {
      enableAnalysisPass = true;
    } else if (arg == "--run-cpu") {
      runCpu = true;
    } else if (arg == "--tensorium-einstein-lower") {
      enableEinsteinLoweringPass = true;
    } else if (arg == "--tensorium-index-analyze") {
      enableIndexAnalyzePass = true;
    } else if (arg == "--tensorium-einstein-validate") {
      enableEinsteinValidityPass = true;
    } else if (arg == "--tensorium-einstein-canonicalize") {
      enableEinsteinCanonicalizePass = true;
    } else if (arg == "--tensorium-einstein-analyze-einsum") {
      enableEinsteinAnalyzeEinsumPass = true;
    } else if (arg == "--dump-mlir") {
      dumpMLIR = true;
    } else if (arg == "--validate") {
      validateOnly = true;
    } else if (arg == "--steps") {
      if (i + 1 >= argc)
        throw std::runtime_error("--steps expects an integer");
      steps = std::stoul(argv[++i]);
    } else if (arg == "--init") {
      if (i + 1 >= argc)
        throw std::runtime_error("--init expects a float");
      initScalar = std::stod(argv[++i]);
    } else if (arg == "--init-alpha") {
      if (i + 1 >= argc)
        throw std::runtime_error("--init-alpha expects a float");
      initAlpha = std::stod(argv[++i]);
    } else {
      files.push_back(arg);
    }
  }

  if (files.empty()) {
    std::cerr << "error: no input files\n";
    return 1;
  }

  try {
    for (const auto &path : files) {
      std::cout << "[Tensorium] parsing " << path << "\n";

      std::string src = readFile(path);

      Lexer lex(src.c_str());
      Parser parser(lex);
      Program prog = parser.parseProgram();

      SemanticAnalyzer sem(prog);
      std::vector<IndexedEvolution> indexedEvos;

      if (dumpIndexed) {
        for (const auto &evo : prog.evolutions) {
          indexedEvos.push_back(sem.analyzeEvolution(evo));
        }
      }
      if (dumpAST) {
        std::cout << "\n=== AST DUMP (" << path << ") ===\n";
        printProgram(prog);
        std::cout << "==============================\n";
      }

      if (dumpIndexed) {
        std::cout << "\n=== INDEXED AST (" << path << ") ===\n";

        for (const auto &evo : indexedEvos) {
          std::cout << "Evolution " << evo.name << " {\n";

          for (const auto &eq : evo.equations) {
            std::cout << "  dt " << eq.fieldName;

            if (!eq.indices.empty()) {
              std::cout << "[";
              for (size_t i = 0; i < eq.indices.size(); ++i) {
                std::cout << eq.indices[i];
                if (i + 1 < eq.indices.size())
                  std::cout << ",";
              }
              std::cout << "]";
            }

            std::cout << " = ";
            printIndexedExpr(eq.rhs.get());
            std::cout << "\n";
          }

          std::cout << "}\n";
        }

        std::cout << "==============================\n";
      }
      auto mod = tensorium::backend::BackendBuilder::build(prog, sem);
      if (validateOnly) {
        auto result = tensorium::sema::validateProgram(mod);

        for (const auto &d : result.diags) {
          std::cerr << (d.kind == tensorium::sema::Diagnostic::Kind::Error
                            ? "error: "
                            : "warning: ")
                    << d.message << "\n";
        }

        if (!result.ok())
          return 1;

        std::cout << "[Tensorium] validation OK: " << path << "\n";
        continue;
      }
      if (dumpBackend) {
        std::cout << "\n=== BACKEND IR (" << path << ") ===\n";
        if (mod.simulation) {
          std::cout << "Simulation:\n";
          std::cout << "  dim = " << mod.simulation->dimension << "\n";
          std::cout << "  dt  = " << mod.simulation->time.dt << "\n";
        }

        std::cout << "Fields:\n";
        for (const auto &f : mod.fields) {
          std::cout << "  " << f.name << " (up=" << f.up << ",down=" << f.down
                    << ")\n";
        }

        std::cout << "Evolutions:\n";
        for (const auto &evo : mod.evolutions) {
          std::cout << "  Evolution " << evo.name << " ("
                    << evo.equations.size() << " eqs)\n";
        }
        std::cout << "==============================\n";
      }
      if (dumpBackendExpr) {
        std::cout << "\n=== BACKEND IR FULL (" << path << ") ===\n";
        tensorium::backend::printModuleIR(mod);
        std::cout << "==============================\n";
      }
      if (dumpMLIR) {
        std::cout << "\n=== MLIR DUMP (" << path << ") ===\n";

        tensorium_mlir::MLIRGenOptions opts;
        opts.enableNoOpPass = enableNoOpPass;
        opts.enableAnalysisPass = enableAnalysisPass;

        opts.enableEinsteinLoweringPass = enableEinsteinLoweringPass;
        opts.enableEinsteinValidityPass = enableEinsteinValidityPass;
        opts.enableIndexAnalyzePass = enableIndexAnalyzePass;
        opts.enableEinsteinCanonicalizePass = enableEinsteinCanonicalizePass;
		opts.enableEinsteinAnalyzeEinsumPass = enableEinsteinAnalyzeEinsumPass;
        tensorium_mlir::emitMLIR(mod, opts);
        std::cout << "==============================\n";
      }
      if (runCpu) {
        tensorium::runtime::RunOptions opt;
        opt.steps = steps;

        auto st = tensorium::runtime::initState1D(mod, initScalar, initAlpha);
        tensorium::runtime::runEuler1D(mod, st, opt);

        for (const auto &kv : st.fields) {
          std::cout << "\n[CPU] Field " << kv.first << " first values: ";
          for (size_t i = 0; i < kv.second.size() && i < 8; ++i) {
            std::cout << kv.second[i] << " ";
          }
          std::cout << "\n";
        }
      }
      std::cout << "[Tensorium] OK: " << path << "\n";
    }

  } catch (const std::exception &e) {
    std::cerr << "Tensorium error: " << e.what() << "\n";
    return 1;
  }

  return 0;
}


================================================================================
FILE: tools/Tester/CMakeLists.txt
================================================================================
add_executable(Tensorium_tester
  main.cpp
  Printer/Printer.cpp
)

target_link_libraries(Tensorium_tester
  PRIVATE tensorium
)


================================================================================
FILE: tools/Tester/Printer/Printer.cpp
================================================================================
#include "tensorium/AST/ASTPrinter.hpp"
#include "tensorium/Lex/Lexer.hpp"
#include "tensorium/Parse/Parser.hpp"
#include "tensorium/Sema/Sema.hpp"
#include <iostream>
#include <vector>

namespace tensorium {

void printField(const FieldDecl &f) {
  std::cout << "field ";

  switch (f.kind) {
  case TensorKind::Scalar:
    std::cout << "scalar ";
    break;
  case TensorKind::Vector:
    std::cout << "vector ";
    break;
  case TensorKind::Covector:
    std::cout << "covector ";
    break;
  case TensorKind::CovTensor2:
    std::cout << "cov_tensor2 ";
    break;
  case TensorKind::ConTensor2:
    std::cout << "con_tensor2 ";
    break;
  case TensorKind::MixedTensor:
    std::cout << "mixed_tensor ";
    break;
  }
  std::cout << f.name << "  (up=" << f.up << ", down=" << f.down << ")";
  if (!f.indices.empty()) {
    std::cout << "[";
    for (size_t i = 0; i < f.indices.size(); ++i) {
      std::cout << f.indices[i];
      if (i + 1 < f.indices.size())
        std::cout << ",";
    }
    std::cout << "]";
  }
  std::cout << "\n";
}

void printSimulation(const SimulationConfig &sim) {
  std::cout << "\n=== Simulation ===\n";

  std::cout << "Coordinates: ";
  switch (sim.coordinates) {
  case CoordinateSystem::Cartesian:
    std::cout << "cartesian";
    break;
  case CoordinateSystem::Spherical:
    std::cout << "spherical";
    break;
  case CoordinateSystem::Cylindrical:
    std::cout << "cylindrical";
    break;
  }
  std::cout << "\n";

  std::cout << "Dimension: " << sim.dimension << "\n";

  std::cout << "Resolution: [";
  for (size_t i = 0; i < sim.resolution.size(); ++i) {
    std::cout << sim.resolution[i];
    if (i + 1 < sim.resolution.size())
      std::cout << ",";
  }
  std::cout << "]\n";

  std::cout << "Time:\n";
  std::cout << "  dt = " << sim.time.dt << "\n";
  std::cout << "  integrator = ";
  switch (sim.time.integrator) {
  case TimeIntegrator::Euler:
    std::cout << "euler";
    break;
  case TimeIntegrator::RK3:
    std::cout << "rk3";
    break;
  case TimeIntegrator::RK4:
    std::cout << "rk4";
    break;
  }
  std::cout << "\n";

  std::cout << "Spatial:\n";
  std::cout << "  scheme = ";
  switch (sim.spatial.scheme) {
  case SpatialScheme::FiniteDifference:
    std::cout << "fd";
    break;
  case SpatialScheme::Spectral:
    std::cout << "spectral";
    break;
  }
  std::cout << "\n";

  std::cout << "  derivative = ";
  switch (sim.spatial.derivative) {
  case DerivativeScheme::Centered:
    std::cout << "centered";
    break;
  case DerivativeScheme::Upwind:
    std::cout << "upwind";
    break;
  }
  std::cout << "\n";

  std::cout << "  order = " << sim.spatial.order << "\n";
}

void printMetric(const MetricDecl &m, int idx) {
  std::cout << "\n=== Metric #" << idx << " ===\n";
  std::cout << "Metric name: " << m.name << "\n";
  std::cout << "Header indices: ";
  for (const auto &id : m.indices)
    std::cout << id << " ";
  std::cout << "\n";

  std::cout << "Number of entries: " << m.entries.size() << "\n";
  for (const auto &e : m.entries) {
    std::cout << "  ";
    std::cout << e.lhs.base;
    if (!e.lhs.indices.empty()) {
      std::cout << "(";
      for (size_t i = 0; i < e.lhs.indices.size(); ++i) {
        std::cout << e.lhs.indices[i];
        if (i + 1 < e.lhs.indices.size())
          std::cout << ",";
      }
      std::cout << ")";
    }
    std::cout << " = ";
    if (e.rhs)
      printExpr(e.rhs.get());
    else
      std::cout << "<null>";
    std::cout << "\n";
  }
}

void printIndexedExpr(const IndexedExpr *e);
void printIndexedBinary(const IndexedBinary *b) {
  std::cout << "(";
  printIndexedExpr(b->lhs.get());
  std::cout << " " << b->op << " ";
  printIndexedExpr(b->rhs.get());
  std::cout << ")";
}

void printIndexedExpr(const IndexedExpr *e) {
  if (auto n = dynamic_cast<const IndexedNumber *>(e)) {
    std::cout << n->value;
    return;
  }

  if (auto v = dynamic_cast<const IndexedVar *>(e)) {
    std::cout << v->name;

    if (!v->tensorIndexNames.empty()) {
      std::cout << "[";
      for (size_t i = 0; i < v->tensorIndexNames.size(); ++i) {
        std::cout << v->tensorIndexNames[i];
        if (i + 1 < v->tensorIndexNames.size())
          std::cout << ",";
      }
      std::cout << "]";
      return;
    }

    std::cout << "[";

    switch (v->kind) {
    case IndexedVarKind::Coordinate:
      std::cout << "coord:" << v->coordIndex;
      break;
    case IndexedVarKind::Local:
      std::cout << "local";
      break;
    case IndexedVarKind::Field:
      std::cout << "field";
      break;
    case IndexedVarKind::Parameter:
      std::cout << "param";
      break;
    }

    if (v->up > 0 || v->down > 0) {
      std::cout << ", tensor(up=" << v->up << ",down=" << v->down << ")";
    }

    std::cout << "]";
    return;
  }

  if (auto b = dynamic_cast<const IndexedBinary *>(e)) {
    printIndexedBinary(b);
    return;
  }

  if (auto c = dynamic_cast<const IndexedCall *>(e)) {
    std::cout << c->callee << "(";
    for (size_t i = 0; i < c->args.size(); ++i) {
      printIndexedExpr(c->args[i].get());
      if (i + 1 < c->args.size())
        std::cout << ", ";
    }
    std::cout << ")";
    return;
  }

  std::cout << "<unknown>";
}

void printIndexedMetric(const IndexedMetric &m) {
  std::cout << "\n=== Indexed Metric ===\n";
  std::cout << m.name << " (rank=" << m.rank << ")\n";

  for (const auto &a : m.assignments) {
    std::cout << "  " << a.tensor;
    if (!a.indexOffsets.empty()) {
      std::cout << "(";
      for (size_t i = 0; i < a.indexOffsets.size(); ++i) {
        std::cout << a.indexOffsets[i];
        if (i + 1 < a.indexOffsets.size())
          std::cout << ",";
      }
      std::cout << ")";
    }
    std::cout << " = ";
    printIndexedExpr(a.rhs.get());
    std::cout << "\n";
  }
}

void printIndexedEvolution(const IndexedEvolution &evo) {
  std::cout << "\n=== Indexed Evolution (" << evo.name << ") ===\n";
  for (const auto &eq : evo.equations) {
    std::cout << "  dt " << eq.fieldName;
    if (!eq.indices.empty()) {
      std::cout << "[";
      for (size_t i = 0; i < eq.indices.size(); ++i) {
        std::cout << eq.indices[i];
        if (i + 1 < eq.indices.size())
          std::cout << ",";
      }
      std::cout << "]";
    }
    std::cout << " = ";
    printIndexedExpr(eq.rhs.get());
    std::cout << "\n";
  }
}

void printEvolution(const EvolutionDecl &evo, int idx) {
  std::cout << "\n=== Evolution #" << idx << " (" << evo.name << ") ===\n";

  for (const auto &eq : evo.equations) {
    std::cout << "  dt " << eq.fieldName;
    if (!eq.indices.empty()) {
      std::cout << "[";
      for (size_t i = 0; i < eq.indices.size(); ++i) {
        std::cout << eq.indices[i];
        if (i + 1 < eq.indices.size())
          std::cout << ",";
      }
      std::cout << "]";
    }
    std::cout << " = ";
    printExpr(eq.rhs.get());
    std::cout << "\n";
  }

  if (!evo.tempAssignments.empty()) {
    std::cout << "  -- Locals --\n";
    for (const auto &tmp : evo.tempAssignments) {
      std::cout << "  " << tmp.lhs.base;
      if (!tmp.lhs.indices.empty()) {
        std::cout << "[";
        for (size_t i = 0; i < tmp.lhs.indices.size(); ++i) {
          std::cout << tmp.lhs.indices[i];
          if (i + 1 < tmp.lhs.indices.size())
            std::cout << ",";
        }
        std::cout << "]";
      }
      std::cout << " = ";
      printExpr(tmp.rhs.get());
      std::cout << "\n";
    }
  }
}

} // namespace tensorium


================================================================================
FILE: tools/Tester/Printer/Printer.hpp
================================================================================
#pragma once

#include "tensorium/AST/AST.hpp"
#include "tensorium/AST/IndexedAST.hpp"
#include <iostream>

namespace tensorium {

void printField(const FieldDecl &f);

void printSimulation(const SimulationConfig &sim);

void printMetric(const MetricDecl &m, int idx);
void printIndexedMetric(const IndexedMetric &m);

void printEvolution(const EvolutionDecl &evo, int idx);
void printIndexedEvolution(const IndexedEvolution &evo);

void printIndexedExpr(const IndexedExpr *e);

} // namespace tensorium


================================================================================
FILE: tools/Tester/main.cpp
================================================================================
#include "Printer/Printer.hpp"
#include "tensorium/AST/ASTPrinter.hpp"
#include "tensorium/Lex/Lexer.hpp"
#include "tensorium/Parse/Parser.hpp"
#include "tensorium/Sema/Sema.hpp"
#include <iostream>
#include <vector>

using namespace tensorium;

struct TestCase {
  std::string name;
  std::string input;
  bool expectFailure = false;
};

bool runTest(const TestCase &t) {
  try {
    Lexer lex(t.input.c_str());
    Parser parser(lex);
    Program prog = parser.parseProgram();
    SemanticAnalyzer sem(prog);

    std::vector<IndexedMetric> indexedMetrics;
    std::vector<IndexedEvolution> indexedEvos;

    for (auto &m : prog.metrics)
      indexedMetrics.push_back(sem.analyzeMetric(m));

    for (auto &e : prog.evolutions)
      indexedEvos.push_back(sem.analyzeEvolution(e));

    if (t.expectFailure) {
      std::cerr << " FAIL (unexpected success): " << t.name << "\n";
      return false;
    }

    std::cout << " PASS: " << t.name << "\n\n";

    std::cout << "========== Pretty-print ==========\n";
    std::cout << "Fields:\n";
    for (auto &f : prog.fields)
      printField(f);
    if (prog.simulation) {
      printSimulation(*prog.simulation);
    }
    for (size_t i = 0; i < prog.metrics.size(); i++) {
      printMetric(prog.metrics[i], (int)i);
      printIndexedMetric(indexedMetrics[i]);
    }

    for (size_t i = 0; i < prog.evolutions.size(); i++) {
      printEvolution(prog.evolutions[i], (int)i);
      printIndexedEvolution(indexedEvos[i]);
    }
    std::cout << "==================================\n\n";

    return true;

  } catch (const std::exception &ex) {
    if (t.expectFailure) {
      std::cout << "✔ PASS (expected failure): " << t.name << " -- "
                << ex.what() << "\n";
      return true;
    }
    std::cerr << " FAIL: " << t.name << " -- " << ex.what() << "\n";
    return false;
  }
}

int main() {

  std::vector<TestCase> tests = {

      {"Valid BSSN evolution",
       R"(
            field scalar chi
            field cov_tensor2 gamma[i,j]
            field cov_tensor2 Atilde[i,j]
            field scalar alpha

            metric g(t,r,theta,phi) {
                rho2 = r^2 + a^2 * cos(theta)^2
                g(t,t) = -(1 - 2*M/r)
            }

            evolution BSSN {
                dt chi        = -2 * alpha * K
                dt gamma[i,j] = -2 * alpha * Atilde[i,j]
                Atilde[i,j]   = contract(gamma[i,k] * Atilde[k,j])
            }
        )"},

      {"Scalar evolution OK",
       R"(
            field scalar phi

            evolution Test {
                dt phi = 2 * phi
            }
        )"},

      {
          "Correct nested contraction",
          R"(
		    field cov_tensor2 A[i,j]
	
		    evolution OK {
				dt A[i,j] = contract(A[i,k] * (A[k,l] * A[l,j]))
			}
		)",
      },

      {"Local temporary reuse OK",
       R"(
            field scalar chi

            evolution Temp {
                dt chi = K
                K = chi * chi
            }
        )"},

      {"Metric-only parameters allowed",
       R"(
            field scalar rho

            metric g(t,r) {
                test = r + t
            }

            evolution OK {
                dt rho = r
            }
        )"},

      {"Invalid index",
       R"(
            field cov_tensor2 gamma[i,j]

            evolution Wrong {
                dt gamma[i,j] = gamma[i,p]  # p not allowed
            }
        )",
       true},

      {"Missing contraction",
       R"(
            field cov_tensor2 gamma[i,j]

            evolution Wrong {
                dt gamma[i,j] = gamma[i,k]  # k appears only on RHS
            }
        )",
       true},

      {"Too many repeated indices (illegal contraction)",
       R"(
            field cov_tensor2 A[i,j]

            evolution Bad {
                dt A[i,j] = A[i,k] * A[k,k] * A[k,j]
            }
        )",
       true},

      {"Wrong tensor rank - vector indexed like matrix",
       R"(
            field vector beta[i]

            evolution Bad {
                dt beta[i,j] = beta[i]
            }
        )",
       true},

      {"Local variable shadowing error",
       R"(
            field scalar chi

            evolution Bad {
                dt chi = alpha
                chi = 5
            }
        )",
       true},

      {"Function call misused in tensor context",
       R"(
            field cov_tensor2 gamma[i,j]
            field cov_tensor2 Atilde[i,j]

            evolution Bad {
                dt gamma[i,j] = sin(gamma)
            }
        )",
       true},

      {"Reference to undeclared field",
       R"(
            field scalar phi

            evolution Bad {
                dt psi = phi
            }
        )",
       true},

      {"Index missing on RHS",
       R"(
            field cov_tensor2 T[i,j]

            evolution Bad {
                dt T[i,j] = T[i,k]
            }
        )",
       true},
      {
          "Partial derivative on scalar -> covector",
          R"(
        field scalar phi
        field covector grad_phi[i]

        evolution Diff {
            dt grad_phi[i] = d_i(phi)
        }
    )",
      },
      {"Laplacian expects scalar",
       R"(
        field covector v[i]

        evolution BadLap {
            dt v[i] = laplacian(v)
        }
    )",
       true},
      {"Covariant derivative of scalar",
       R"(
		field scalar phi
		field covector dphi[i]

		evolution OK {
			dt dphi[i] = nabla_i(phi)
		}
	)"},

      {"Contravariant covariant derivative without inverse metric",
       R"(
    field scalar phi
    field vector v[i]

    evolution Bad {
      dt v[i] = nabla^i(phi)
    }
  )",
       true},

      {"Simulation block with time and spatial config",
       R"(
    field scalar phi

    simulation {
      coordinates = cartesian
      dimension = 3
      resolution = [64,64,64]

      time {
        dt = 0.01
        integrator = rk4
      }

      spatial {
        scheme = fd
        derivative = centered
        order = 4
      }
    }

    evolution Test {
      dt phi = phi
    }
  )"},
      {"Invalid simulation: resolution mismatch",
       R"(
    field scalar phi

    simulation {
      dimension = 3
      resolution = [64,64]

      time { dt = 0.01 integrator = rk4 }
      spatial { scheme = fd derivative = centered order = 4 }
    }

    evolution Test { dt phi = phi }
  )",
       true},
      {"Invalid simulation: odd FD order",
       R"(
    field scalar phi

    simulation {
      dimension = 3
      resolution = [32,32,32]

      time { dt = 0.01 integrator = rk4 }
      spatial { scheme = fd derivative = centered order = 3 }
    }

    evolution Test { dt phi = phi }
  )",
       true},
  };

  bool ok = true;
  for (auto &t : tests)
    ok &= runTest(t);

  std::cout << "\n=== FINAL TEST STATUS: " << (ok ? "ALL PASSED ✔" : "FAIL ")
            << "\n";

  return ok ? 0 : 1;
}


================================================================================
FILE: include/tensorium_mlir/Init/Registry.h
================================================================================
#pragma once

#include "mlir/IR/DialectRegistry.h"

namespace tensorium_mlir {
void registerAllDialects(mlir::DialectRegistry &registry);
} // namespace tensorium_mlir


================================================================================
FILE: include/tensorium_mlir/Init/Passes.h
================================================================================
#pragma once

namespace tensorium_mlir {
void registerAllPasses();
} // namespace tensorium_mlir


================================================================================
FILE: include/tensorium_mlir/Pipeline/Pipeline.h
================================================================================
#pragma once

#include "mlir/Pass/PassManager.h"
namespace tensorium_mlir {
struct PipelineOptions {
  bool verify = true;
  bool printIR = false;
};
void buildDefaultPipeline(mlir::PassManager &pm, const PipelineOptions &opt);

} // namespace tensorium_mlir


================================================================================
FILE: include/tensorium_mlir/Target/LLVMIR/Export.h
================================================================================


================================================================================
FILE: include/tensorium_mlir/Target/MLIRGen/MLIRGen.h
================================================================================

#pragma once
#include "tensorium/Backend/DomainIR.hpp"

namespace tensorium_mlir {

struct MLIRGenOptions {
  bool enableNoOpPass = false;
  bool enableAnalysisPass = false;
  bool enableEinsteinLoweringPass = false;
  bool enableIndexRoleAnalysisPass = false;
  bool enableEinsteinValidityPass = false;
  bool enableIndexAnalyzePass = false;
  bool enableEinsteinCanonicalizePass = false;
  bool enableEinsteinAnalyzeEinsumPass = false;
};

void emitMLIR(const tensorium::backend::ModuleIR &module,
              const MLIRGenOptions &opts = {});
} // namespace tensorium_mlir


================================================================================
FILE: include/tensorium_mlir/Conversion/TensoriumToLinalg/TensoriumToLinalg.h
================================================================================


================================================================================
FILE: include/tensorium_mlir/Semantic/Einstein.h
================================================================================

#pragma once

#include "llvm/ADT/DenseMap.h"
#include "llvm/ADT/MapVector.h"
#include "llvm/ADT/SmallVector.h"
#include "llvm/ADT/StringRef.h"

#include <cstdint>

namespace tensorium::semantic {

enum class IndexRoleKind : uint8_t {
  Free,
  Contracted,
  Summed,
  Dangling,
  Invalid
};

struct EinsteinAnalysisResult {
  llvm::SmallVector<llvm::SmallVector<llvm::StringRef, 8>, 4> ins;
  llvm::SmallVector<llvm::StringRef, 8> out;
  llvm::SmallVector<llvm::StringRef, 16> all;

  llvm::MapVector<llvm::StringRef, int64_t> counts;

  llvm::DenseMap<llvm::StringRef, IndexRoleKind> roles;

  bool valid = true;
};

struct EinsteinAnalyzeOptions {
  bool allowSummed = false;
  bool allowDangling = false;
};

EinsteinAnalysisResult analyzeEinstein(llvm::ArrayRef<llvm::StringRef> outIdx,
                                       llvm::ArrayRef<llvm::StringRef> rhsIdx,
                                       const EinsteinAnalyzeOptions &opt = {});

llvm::StringRef roleToString(IndexRoleKind r);

} // namespace tensorium::semantic


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/IR/TensoriumTypes.h
================================================================================
#pragma once

#include "mlir/IR/Dialect.h"
#include "mlir/IR/TypeSupport.h"
#include "mlir/IR/Types.h"
#include "llvm/ADT/StringRef.h"

#include <cstdint>
#include <tuple>

namespace tensorium {
namespace mlir {

enum class Variance : uint8_t { Scalar, Contravariant, Covariant, Mixed };

struct FieldTypeStorage : public ::mlir::TypeStorage {
  using KeyTy = std::tuple<::mlir::Type, unsigned, Variance>;

  FieldTypeStorage(::mlir::Type elementType, unsigned rank, Variance variance)
      : elementType(elementType), rank(rank), variance(variance) {}

  bool operator==(const KeyTy &key) const {
    return key == KeyTy(elementType, rank, variance);
  }

  static FieldTypeStorage *construct(::mlir::TypeStorageAllocator &alloc,
                                     const KeyTy &key) {
    return new (alloc.allocate<FieldTypeStorage>())
        FieldTypeStorage(std::get<0>(key), std::get<1>(key), std::get<2>(key));
  }

  ::mlir::Type elementType;
  unsigned rank;
  Variance variance;
};

class FieldType
    : public ::mlir::Type::TypeBase<FieldType, ::mlir::Type, FieldTypeStorage> {
public:
  using Base::Base;

  static constexpr ::llvm::StringLiteral name = "tensorium.field";

  static FieldType get(::mlir::MLIRContext *ctx, ::mlir::Type elementType,
                       unsigned rank, Variance variance);

  ::mlir::Type getElementType() const;
  unsigned getRank() const;
  Variance getVariance() const;
};

} // namespace mlir
} // namespace tensorium


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/IR/TensoriumTypes.td
================================================================================


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.h
================================================================================
#pragma once

#include "mlir/Bytecode/BytecodeOpInterface.h"
#include "mlir/IR/Attributes.h"
#include "mlir/IR/BuiltinTypes.h"
#include "mlir/IR/DialectImplementation.h"
#include "mlir/IR/OpDefinition.h"
#include "mlir/IR/Operation.h"
#include "mlir/IR/Attributes.h" 
#include "mlir/Interfaces/InferTypeOpInterface.h"
#include "mlir/Interfaces/SideEffectInterfaces.h"
#include "mlir/Interfaces/InferTypeOpInterface.h"

namespace tensorium::mlir {
  using Attribute = ::mlir::Attribute; 
}
namespace tensorium {
namespace mlir {
class TensoriumDialect;
} // namespace mlir
} // namespace tensorium

#define GET_OP_CLASSES
#include "TensoriumOps.h.inc"


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.h
================================================================================

#pragma once

#include "mlir/IR/Dialect.h"

namespace mlir {
class DialectAsmParser;
class DialectAsmPrinter;
} // namespace mlir

namespace tensorium {
namespace mlir {

class TensoriumDialect : public ::mlir::Dialect {
public:
  explicit TensoriumDialect(::mlir::MLIRContext *ctx);

  static ::llvm::StringRef getDialectNamespace() { return "tensorium"; }

  ::mlir::Type parseType(::mlir::DialectAsmParser &parser) const override;
  void printType(::mlir::Type type,
                 ::mlir::DialectAsmPrinter &printer) const override;
};

} // namespace mlir
} // namespace tensorium


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/IR/TensoriumDialect.td
================================================================================


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/IR/TensoriumOps.td
================================================================================
include "mlir/IR/AttrTypeBase.td"
include "mlir/IR/DialectBase.td"
include "mlir/IR/OpBase.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"

def Tensorium_Dialect : Dialect {
  let name = "tensorium";
  let cppNamespace = "::tensorium::mlir";
}

class Tensorium_Op<string mnemonic>
    : Op<Tensorium_Dialect, mnemonic>;

def Tensorium_IndexOp : Tensorium_Op<"index"> {
  let summary = "Symbolic tensor indexing";

  let arguments = (ins
    AnyType:$field,
    ArrayAttr:$indices
  );

  let results = (outs
    AnyType:$result
  );

  let assemblyFormat =
    "$field `[` $indices `]` attr-dict `:` type($field) `->` type($result)";

  let hasVerifier = 1;
}

def Tensorium_ConstOp : Tensorium_Op<"const"> {
  let arguments = (ins F64Attr:$value);
  let results = (outs F64:$result);
  let assemblyFormat = "$value attr-dict `:` type($result)";
}

def Tensorium_RefOp : Tensorium_Op<"ref"> {
  let arguments = (ins AnyType:$source, StrAttr:$kind);
  let results = (outs F64:$result);
  let assemblyFormat =
    "$source attr-dict `:` type($source) `->` type($result)";
}

def Tensorium_AddOp : Tensorium_Op<"add"> {
  let traits = [Pure];

  let arguments = (ins F64:$lhs, F64:$rhs);
  let results   = (outs F64:$res);
  let assemblyFormat = "$lhs `,` $rhs attr-dict `:` type($res)";
}

def Tensorium_SubOp : Tensorium_Op<"sub"> {
  let traits = [Pure];

  let arguments = (ins F64:$lhs, F64:$rhs);
  let results   = (outs F64:$res);
  let assemblyFormat = "$lhs `,` $rhs attr-dict `:` type($res)";
}

def Tensorium_MulOp : Tensorium_Op<"mul"> {
  let traits = [Pure];

  let arguments = (ins F64:$lhs, F64:$rhs);
  let results   = (outs F64:$res);
}

def Tensorium_DerivOp : Tensorium_Op<"deriv"> {
  let traits = [Pure];

  let arguments = (ins F64:$in);
  let results   = (outs F64:$out);
  let assemblyFormat = "$in attr-dict `:` type($in) `->` type($out)";
}

def Tensorium_ContractOp : Tensorium_Op<"contract"> {
  let traits = [Pure];

  let arguments = (ins F64:$in);
  let results   = (outs F64:$out);
  let assemblyFormat = "$in attr-dict `:` type($in) `->` type($out)";
}

def Tensorium_DtAssignOp : Tensorium_Op<"dt_assign"> {

  let arguments = (ins AnyType:$field, F64:$rhs, ArrayAttr:$indices);
  let results = (outs);
  let assemblyFormat =
    "$field `,` $rhs attr-dict `:` type($field) `,` type($rhs)";
}


def Tensorium_EinsumOp : Tensorium_Op<"einsum"> {
  let summary = "Einstein summation (tensor contraction)";

  let traits = [Pure];
  let arguments = (ins Variadic<AnyType>:$inputs);
  let results   = (outs AnyType:$result);

  let hasCustomAssemblyFormat = 1;
}





================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/Transform/NoOpPass.h
================================================================================
#pragma once

#include "mlir/Pass/Pass.h"

namespace tensorium {
namespace mlir {

std::unique_ptr<::mlir::Pass> createTensoriumNoOpPass();

} // namespace mlir
} // namespace tensorium


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/Transform/EinsteinValidityPass.h
================================================================================
#pragma once
#include <memory>

namespace mlir {
class Pass;
}

namespace tensorium::mlir {
std::unique_ptr<::mlir::Pass> createTensoriumEinsteinValidityPass();
}


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/Transform/Passes.td
================================================================================
#ifndef TENSORIUM_TRANSFORM_PASSES_TD
#define TENSORIUM_TRANSFORM_PASSES_TD

include "mlir/Pass/PassBase.td"

def TensoriumNoOpPass : Pass<"tensorium-noop", "mlir::ModuleOp"> {
  let summary = "Tensorium no-op pass";
  let description = [{
    No-op pass used to validate the Tensorium MLIR pipeline.
  }];
}

def TensoriumEinsteinLowering : Pass<"tensorium-einstein-lowering", "mlir::ModuleOp"> {
  let summary = "Lower tensorium.contract into tensorium.einsum (Einstein symbolic lowering)";
  let constructor = "tensorium::mlir::createTensoriumEinsteinLoweringPass()";
}
#endif // TENSORIUM_TRANSFORM_PASSES_TD


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/Transform/Passes.h
================================================================================
#pragma once

#include "mlir/Pass/Pass.h"

namespace tensorium {
namespace mlir {

void registerTensoriumTransformPasses();
std::unique_ptr<::mlir::Pass> createTensoriumNoOpPass();
std::unique_ptr<::mlir::Pass> createTensoriumAnalysisPass();
std::unique_ptr<::mlir::Pass> createTensoriumEinsteinLoweringPass();

std::unique_ptr<::mlir::Pass> createTensoriumIndexAnalyzePass();
std::unique_ptr<::mlir::Pass> createTensoriumEinsteinLoweringPass();
std::unique_ptr<::mlir::Pass> createTensoriumEinsteinAnalyzeEinsumPass();
std::unique_ptr<::mlir::Pass> createTensoriumEinsteinValidityPass();
std::unique_ptr<::mlir::Pass> createTensoriumEinsteinCanonicalizePass();
} // namespace mlir
} // namespace tensorium

#include "TensoriumPasses.h.inc"


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/Transform/EinsteinCanonicalizePass.h
================================================================================
#pragma once
#include "mlir/Pass/Pass.h"
#include <memory>

namespace tensorium::mlir {
std::unique_ptr<::mlir::Pass> createTensoriumEinsteinCanonicalizePass();
}


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/Transform/EinsteinAnalyzeEinsumPass.h
================================================================================
#pragma once

#include <memory>

namespace mlir {
class Pass;
} // namespace mlir

namespace tensorium::mlir {

std::unique_ptr<::mlir::Pass> createTensoriumEinsteinAnalyzeEinsumPass();

} // namespace tensorium::mlir


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/Transform/IndexAnalyzePass.h
================================================================================
#pragma once
#include <memory>

namespace mlir {
class Pass;
}

namespace tensorium::mlir {
std::unique_ptr<::mlir::Pass> createTensoriumIndexAnalyzePass();
}


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/Transform/IndexRoleAnalysisPass.h
================================================================================
#pragma once
#include <memory>

namespace mlir {
class Pass;
}

namespace tensorium::mlir {
std::unique_ptr<::mlir::Pass> createTensoriumIndexRoleAnalysisPass();
}


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/Transform/AnalysisPass.h
================================================================================
#pragma once
#include "mlir/Pass/Pass.h"

namespace tensorium {
namespace mlir {

std::unique_ptr<::mlir::Pass> createTensoriumAnalysisPass();

} // namespace mlir
} // namespace tensorium


================================================================================
FILE: include/tensorium_mlir/Dialect/Tensorium/Transform/EinsteinLoweringPass.h
================================================================================
#pragma once
#include <memory>

namespace mlir {
class Pass;
}

namespace tensorium::mlir {
std::unique_ptr<::mlir::Pass> createTensoriumEinsteinLoweringPass();
}


================================================================================
FILE: include/tensorium/Lex/Lexer.hpp
================================================================================
#pragma once
#include "tensorium/Basic/Token.hpp"

namespace tensorium {
class Lexer {
  const char *src;
  int line = 1;
  int col = 1;
  void advanceChar();
public:
  explicit Lexer(const char *input);
  Token next();
};
}


================================================================================
FILE: include/tensorium/Parse/Parser.hpp
================================================================================
#pragma once
#include "tensorium/AST/AST.hpp"
#include "tensorium/Lex/Lexer.hpp"

namespace tensorium {
class Parser {
  Lexer &lex;
  Token cur;
  void advance();
  void expect(TokenType type);
  [[noreturn]] void syntaxError(const std::string &msg);

  std::unique_ptr<Expr> parseExpr();
  std::unique_ptr<Expr> parseAddExpr();
  std::unique_ptr<Expr> parseMulExpr();
  std::unique_ptr<Expr> parsePowExpr();
  std::unique_ptr<Expr> parseUnaryExpr();
  std::unique_ptr<Expr> parsePrimary();
  std::vector<std::unique_ptr<Expr>> parseExprList();

  TensorAccess parseLHS();
  Assignment parseAssignment();
  FieldDecl parseFieldDecl();
  MetricDecl parseMetric();
  EvolutionEq parseEvolutionEq();
  EvolutionDecl parseEvolution();
  SimulationConfig parseSimulation();
  TimeConfig parseTimeBlock();
  SpatialConfig parseSpatialBlock();

public:
  explicit Parser(Lexer &l);
  Program parseProgram();
};
} // namespace tensorium


================================================================================
FILE: include/tensorium/Runtime/Eval.hpp
================================================================================

#pragma once
#include "tensorium/Backend/DomainIR.hpp"
#include <unordered_map>
#include <stdexcept>

namespace tensorium::runtime {

struct ScalarEnv {
  std::unordered_map<std::string, const double*> fieldPtr;
  std::unordered_map<std::string, double> params;
};

double evalScalar(const backend::ExprIR* e, const ScalarEnv& env);

} // namespace tensorium::runtime


================================================================================
FILE: include/tensorium/Runtime/CpuRuntime.hpp
================================================================================

#pragma once
#include "tensorium/Backend/DomainIR.hpp"
#include <cstddef>
#include <unordered_map>
#include <vector>

namespace tensorium::runtime {

struct CpuState1D {
  std::size_t n = 0;
  std::unordered_map<std::string, std::vector<double>> fields;
  std::unordered_map<std::string, double> params;
};

struct RunOptions {
  std::size_t steps = 10;
};

CpuState1D initState1D(const backend::ModuleIR &mod, double initScalar,
                       double initAlpha);

void runEuler1D(const backend::ModuleIR &mod, CpuState1D &st,
                const RunOptions &opt);

} // namespace tensorium::runtime


================================================================================
FILE: include/tensorium/Basic/TensorSignature.hpp
================================================================================
#pragma once
#include <string>
#include <vector>

namespace tensorium {

enum class VarianceKind { Scalar, Contravariant, Covariant, Mixed };

struct TensorSignature {
  int up = 0;
  int down = 0;
  std::vector<std::string> indices;

  int getRank() const { return up + down; }
  bool isScalar() const { return up == 0 && down == 0; }

  VarianceKind getVariance() const {
    if (isScalar())
      return VarianceKind::Scalar;
    if (up > 0 && down == 0)
      return VarianceKind::Contravariant;
    if (up == 0 && down > 0)
      return VarianceKind::Covariant;
    return VarianceKind::Mixed;
  }
};

} // namespace tensorium


================================================================================
FILE: include/tensorium/Basic/Token.hpp
================================================================================
#pragma once
#include <string>

namespace tensorium {
enum class TokenType {
  End,
  Identifier,
  Number,
  LParen,
  RParen,
  LBrace,
  RBrace,
  LBracket,
  RBracket,
  Comma,
  Equals,
  Plus,
  Minus,
  Star,
  Slash,
  Caret,
  KwSpacetime,
  KwMetric,
  KwDecompose,
  KwCoords,
  KwParams,
  KwSignature,
  KwLapse,
  KwShift,
  KwSpatial,
  KwExtrinsic,
  KwField,
  KwScalar,
  KwVector,
  KwTensor2,
  KwCovector,
  KwCovTensor2,
  KwConTensor2,
  KwCovTensor3,
  KwConTensor3,
  KwCovTensor4,
  KwConTensor4,
  KwEq,
  KwEvolution,
  KwDt,
  KwSimulation,
  KwTime,
  Unknown
};

struct Token {
  TokenType type;
  std::string text;
  int line;
  int column;
};
} // namespace tensorium


================================================================================
FILE: include/tensorium/Sema/Sema.hpp
================================================================================
#pragma once
#include "tensorium/AST/AST.hpp"
#include "tensorium/AST/IndexedAST.hpp" // Inclusion du fichier complet
#include <deque>
#include <memory>
#include <unordered_map>
#include <unordered_set>

static const std::unordered_set<std::string> SPATIAL_INDICES = {"i", "j", "k",
                                                                "l", "m", "n"};
namespace tensorium {

class SemanticAnalyzer {
  const Program &prog;
  std::unordered_map<std::string, int> coordIndex;
  std::unordered_map<std::string, bool> locals;
  std::unordered_map<std::string, const FieldDecl *> fields;
  std::vector<FieldDecl> syntheticMetricFields;
  std::unordered_map<std::string, int> indexUseCount;
  std::unordered_set<std::string> lhsIndices;

  void validateSpatialIndex(const std::string &idx);
  int resolveIndex(const std::string &name);
  std::unique_ptr<IndexedExpr> transformExpr(const Expr *e);
  void validateSimulation(const SimulationConfig &sim);

public:
  explicit SemanticAnalyzer(const Program &p);
  IndexedMetric analyzeMetric(const MetricDecl &decl);
  IndexedEvolution analyzeEvolution(const EvolutionDecl &evo);
};
} // namespace tensorium


================================================================================
FILE: include/tensorium/Sema/SymbolTable.hpp
================================================================================
#pragma once
#include "tensorium/Basic/TensorSignature.hpp"
#include <optional>
#include <string>
#include <unordered_map>

namespace tensorium {

enum class SymbolKind { Field, Parameter, Local, Coordinate };

struct Symbol {
  std::string name;
  SymbolKind kind;
  TensorSignature signature;
  int coordIndex = -1;
};

class SymbolTable {
  std::unordered_map<std::string, Symbol> symbols;

public:
  void insert(const Symbol &sym) { symbols[sym.name] = sym; }

  std::optional<Symbol> lookup(const std::string &name) const {
    auto it = symbols.find(name);
    if (it != symbols.end())
      return it->second;
    return std::nullopt;
  }
};

} // namespace tensorium


================================================================================
FILE: include/tensorium/Sema/tensor_type_checker.hpp
================================================================================
#pragma once
#include "tensorium/AST/AST.hpp"
#include "tensorium/AST/IndexedAST.hpp"
#include <stdexcept>
#include <string>

namespace tensorium {

struct TensorType {
  int up = 0;
  int down = 0;

  bool isScalar() const { return up == 0 && down == 0; }
  int rank() const { return up + down; }

  bool sameVariance(const TensorType &o) const {
    return up == o.up && down == o.down;
  }
};

class TensorTypeChecker {
  bool isPartialDerivative(const std::string &name) const {
    if (name.size() != 3)
      return false;
    if (name[0] != 'd' || name[1] != '_')
      return false;
    char c = name[2];
    return (c == 'i' || c == 'j' || c == 'k' || c == 'l' || c == 'm' ||
            c == 'n');
  }

  bool isScalarExpr(const IndexedExpr *e) const {
    try {
      return inferImpl(e, true).isScalar();
    } catch (...) {
      return false;
    }
  }

  bool isCovariantDerivative(const std::string &name, bool &contravariant,
                             char &index) const {
    if (name.size() == 7 && name.rfind("nabla_", 0) == 0) {
      index = name[6];
      contravariant = false;
      return true;
    }
    if (name.size() == 7 && name.rfind("nabla^", 0) == 0) {
      index = name[6];
      contravariant = true;
      return true;
    }
    return false;
  }

  static bool isTensorIndexChar(char c) {
    return (c == 'i' || c == 'j' || c == 'k' || c == 'l' || c == 'm' ||
            c == 'n');
  }

  void collectIndexCounts(const IndexedExpr *e, int counts[256]) const {
    if (!e)
      return;

    if (auto v = dynamic_cast<const IndexedVar *>(e)) {
      for (const auto &name : v->tensorIndexNames) {
        if (!name.empty()) {
          char c = name[0];
          if (isTensorIndexChar(c))
            counts[(unsigned char)c]++;
        }
      }
      return;
    }

    if (auto b = dynamic_cast<const IndexedBinary *>(e)) {
      collectIndexCounts(b->lhs.get(), counts);
      collectIndexCounts(b->rhs.get(), counts);
      return;
    }

    if (auto c = dynamic_cast<const IndexedCall *>(e)) {
      const std::string &cal = c->callee;

      if (cal == "contract") {
        if (c->args.size() != 1)
          throw std::runtime_error("contract() expects 1 argument");

        int tmp[256] = {0};
        collectIndexCounts(c->args[0].get(), tmp);

        for (char idx : {'i', 'j', 'k', 'l', 'm', 'n'}) {
          int cc = tmp[(unsigned char)idx];
          if (cc == 0)
            continue;
          if (cc == 1) {
            counts[(unsigned char)idx]++;
            continue;
          }
          if (cc == 2)
            continue;
          throw std::runtime_error(
              std::string("Ambiguous contraction: index '") + idx +
              "' appears " + std::to_string(cc) + " times.");
        }
        return;
      }

      for (const auto &arg : c->args)
        collectIndexCounts(arg.get(), counts);

      if (isPartialDerivative(cal)) {
        char idx = cal[2];
        counts[(unsigned char)idx]++;
        return;
      }

      bool contra = false;
      char nidx = 0;
      if (isCovariantDerivative(cal, contra, nidx)) {
        counts[(unsigned char)nidx]++;
        return;
      }

      return;
    }
  }

  void collectAdditiveTerms(const IndexedExpr *e,
                            std::vector<const IndexedExpr *> &out) const {
    if (!e)
      return;

    if (auto b = dynamic_cast<const IndexedBinary *>(e)) {
      if (b->op == '+' || b->op == '-') {
        collectAdditiveTerms(b->lhs.get(), out);
        collectAdditiveTerms(b->rhs.get(), out);
        return;
      }

      if (b->op == '*') {
        const IndexedExpr *L = b->lhs.get();
        const IndexedExpr *R = b->rhs.get();
        if (isScalarExpr(L)) {
          collectAdditiveTerms(R, out);
          return;
        }
        if (isScalarExpr(R)) {
          collectAdditiveTerms(L, out);
          return;
        }
      }

      if (b->op == '/') {
        const IndexedExpr *R = b->rhs.get();
        if (isScalarExpr(R)) {
          collectAdditiveTerms(b->lhs.get(), out);
          return;
        }
      }
    }

    out.push_back(e);
  }

public:
  TensorType inferImpl(const IndexedExpr *e, bool allowRepeated) const {
    if (!e)
      throw std::runtime_error("null expression in tensor type inference");

    if (dynamic_cast<const IndexedNumber *>(e))
      return TensorType{0, 0};

    if (auto v = dynamic_cast<const IndexedVar *>(e)) {
      if (!allowRepeated) {
        std::unordered_set<std::string> seen;
        for (const auto &name : v->tensorIndexNames) {
          if (!name.empty()) {
            if (!seen.insert(name).second) {
              throw std::runtime_error("Implicit trace '" + v->name + "[" +
                                       name + "," + name +
                                       "]' is forbidden; use explicit trace()");
            }
          }
        }
      }

      switch (v->tensorKind) {
      case TensorKind::Scalar:
        return TensorType{0, 0};
      case TensorKind::Vector:
        return TensorType{1, 0};
      case TensorKind::Covector:
        return TensorType{0, 1};
      case TensorKind::CovTensor2:
        return TensorType{0, 2};
      case TensorKind::ConTensor2:
        return TensorType{2, 0};
      case TensorKind::CovTensor3:
        return TensorType{0, 3};
      case TensorKind::ConTensor3:
        return TensorType{3, 0};
      case TensorKind::CovTensor4:
        return TensorType{0, 4};
      case TensorKind::ConTensor4:
        return TensorType{4, 0};
      case TensorKind::MixedTensor:
        return TensorType{v->up, v->down};
      }
    }

    if (auto b = dynamic_cast<const IndexedBinary *>(e)) {
      TensorType lt = inferImpl(b->lhs.get(), allowRepeated);
      TensorType rt = inferImpl(b->rhs.get(), allowRepeated);

      if (b->op == '+' || b->op == '-') {
        if (!lt.sameVariance(rt))
          throw std::runtime_error(
              "tensor addition/subtraction requires identical variance");
        return lt;
      }

      if (b->op == '*')
        return TensorType{lt.up + rt.up, lt.down + rt.down};

      if (b->op == '/') {
        if (!rt.isScalar())
          throw std::runtime_error(
              "division by non-scalar tensor is not allowed");
        return lt;
      }

      return lt;
    }

    if (auto call = dynamic_cast<const IndexedCall *>(e)) {
      const std::string &cal = call->callee;

      if (cal == "contract") {
        if (call->args.size() != 1)
          throw std::runtime_error("contract() expects 1 argument");

        const IndexedExpr *arg = call->args[0].get();
        TensorType t = inferImpl(arg, true);

        int counts[256] = {0};
        collectIndexCounts(arg, counts);

        int freeCount = 0;
        int contracted = 0;

        for (char idx : {'i', 'j', 'k', 'l', 'm', 'n'}) {
          int c = counts[(unsigned char)idx];
          if (c == 0)
            continue;
          if (c == 1) {
            freeCount += 1;
            continue;
          }
          if (c == 2) {
            contracted += 1;
            continue;
          }
          throw std::runtime_error(
              std::string("Ambiguous contraction: index '") + idx +
              "' appears " + std::to_string(c) + " times.");
        }

        if (contracted == 0)
          throw std::runtime_error(
              "contract() expects at least one repeated index");

        if (t.up == 0 && t.down > 0)
          return TensorType{0, freeCount};

        if (t.down == 0 && t.up > 0)
          return TensorType{freeCount, 0};

        int remove = 2 * contracted;
        int up = t.up;
        int down = t.down;

        int rem = remove;
        int takeDown = (down < rem) ? down : rem;
        down -= takeDown;
        rem -= takeDown;

        int takeUp = (up < rem) ? up : rem;
        up -= takeUp;
        rem -= takeUp;

        if (rem != 0)
          throw std::runtime_error(
              "internal error: contract() could not remove requested rank");

        return TensorType{up, down};
      }

      if (isPartialDerivative(cal)) {
        if (call->args.size() != 1)
          throw std::runtime_error("d_* expects exactly 1 argument");
        TensorType argT = inferImpl(call->args[0].get(), allowRepeated);
        return TensorType{argT.up, argT.down + 1};
      }

      bool contra = false;
      char idx = 0;
      if (isCovariantDerivative(cal, contra, idx)) {
        if (call->args.size() != 1)
          throw std::runtime_error("nabla expects exactly 1 argument");
        TensorType t = inferImpl(call->args[0].get(), allowRepeated);
        if (contra)
          return TensorType{t.up + 1, t.down};
        return TensorType{t.up, t.down + 1};
      }

      if (cal == "laplacian") {
        if (call->args.size() != 1)
          throw std::runtime_error("laplacian() expects exactly 1 argument");
        TensorType argT = inferImpl(call->args[0].get(), allowRepeated);
        if (!argT.isScalar())
          throw std::runtime_error("laplacian() expects scalar argument");
        return TensorType{0, 0};
      }

      for (auto &arg : call->args) {
        TensorType t = inferImpl(arg.get(), allowRepeated);
        if (!t.isScalar())
          throw std::runtime_error("function '" + cal +
                                   "' expects scalar argument");
      }

      return TensorType{0, 0};
    }

    throw std::runtime_error("unsupported expression in tensor type inference");
  }

  TensorType infer(const IndexedExpr *e) const { return inferImpl(e, false); }
  void checkAssignmentVariance(const TensorType &lhs,
                               const std::vector<std::string> &lhsIndexNames,
                               const IndexedExpr *rhs) const {
    TensorType rhsRaw = infer(rhs);

    bool lhsSet[256] = {false};
    for (const auto &nm : lhsIndexNames) {
      if (nm.empty())
        continue;
      char c = nm[0];
      if (!isTensorIndexChar(c))
        throw std::runtime_error("Invalid tensor index '" + nm + "'");
      lhsSet[(unsigned char)c] = true;
    }

    std::vector<const IndexedExpr *> terms;
    collectAdditiveTerms(rhs, terms);

    for (const IndexedExpr *t : terms) {
      int counts[256] = {0};
      collectIndexCounts(t, counts);

      for (char idx : {'i', 'j', 'k', 'l', 'm', 'n'}) {
        int c = counts[(unsigned char)idx];
        bool inLhs = lhsSet[(unsigned char)idx];

        if (c == 0)
          continue;

        if (c >= 3) {
          throw std::runtime_error(
              std::string("Ambiguous contraction: index '") + idx +
              "' appears " + std::to_string(c) + " times.");
        }

        if (inLhs) {
          if (c != 1) {
            throw std::runtime_error(
                std::string("Invalid Einstein: LHS index '") + idx +
                "' must appear exactly once in RHS.");
          }
        } else {
          if (c == 1) {
            throw std::runtime_error(std::string("Free index '") + idx +
                                     "' appears only in RHS and not LHS.");
          }
        }
      }
    }

    // Variance/type check: rely on infer(rhs) which already accounts for
    // contract() and derivative arity rules.
    TensorType rhsEff = rhsRaw;

    if (!lhs.sameVariance(rhsEff)) {
      throw std::runtime_error(
          "tensor assignment mismatch: LHS(" + std::to_string(lhs.up) + "," +
          std::to_string(lhs.down) + ") vs RHS(" + std::to_string(rhsEff.up) +
          "," + std::to_string(rhsEff.down) + ")");
    }
  }

  void checkMetricAssignment(const IndexedAssignment &a) const {
    TensorType t = infer(a.rhs.get());
    if (!t.isScalar()) {
      throw std::runtime_error("metric assignment to '" + a.tensor +
                               "' must be scalar (got tensor rank=" +
                               std::to_string(t.rank()) + ")");
    }
  }
};

} // namespace tensorium


================================================================================
FILE: include/tensorium/Sema/ProgramValidator.hpp
================================================================================
#pragma once
#include "tensorium/Backend/DomainIR.hpp"
#include <string>
#include <vector>

namespace tensorium::sema {

struct Diagnostic {
  enum class Kind { Error, Warning };
  Kind kind;
  std::string message;
};

struct ValidationResult {
  std::vector<Diagnostic> diags;
  bool ok() const {
    for (auto &d : diags)
      if (d.kind == Diagnostic::Kind::Error)
        return false;
    return true;
  }
};

ValidationResult validateProgram(const backend::ModuleIR &m);

} // namespace tensorium::sema


================================================================================
FILE: include/tensorium/Backend/DomainIR.hpp
================================================================================

#pragma once
#include <cstdint>
#include <memory>
#include <optional>
#include <string>
#include <utility>
#include <vector>

namespace tensorium::backend {

enum class CoordSystem { Cartesian, Spherical, Cylindrical };
enum class TimeIntegrator { Euler, RK3, RK4 };
enum class SpatialScheme { FD, Spectral };
enum class DerivativeScheme { Centered, Upwind };

struct TimeIR {
  double dt = 0.0;
  TimeIntegrator integrator = TimeIntegrator::Euler;
};

struct SpatialIR {
  SpatialScheme scheme = SpatialScheme::FD;
  DerivativeScheme derivative = DerivativeScheme::Centered;
  int order = 2;
};

struct SimulationIR {
  CoordSystem coords = CoordSystem::Cartesian;
  int dimension = 0;
  std::vector<int> resolution;
  TimeIR time;
  SpatialIR spatial;
};

enum class FieldKind {
  Scalar,
  Vector,
  Covector,
  CovTensor2,
  ConTensor2,
  CovTensor3,
  ConTensor3,
  CovTensor4,
  ConTensor4,
  MixedTensor
};

struct FieldIR {
  std::string name;
  FieldKind kind = FieldKind::Scalar;
  int up = 0;
  int down = 0;
};

enum class VarKind { Field, Param, Local, Coord };

struct ExprIR {
  enum class Kind { Number, Var, Binary, Call };
  Kind kind;

  virtual ~ExprIR() = default;
  explicit ExprIR(Kind k) : kind(k) {}
};

struct NumberIR final : ExprIR {
  double value;
  explicit NumberIR(double v) : ExprIR(Kind::Number), value(v) {}
};

struct VarIR final : ExprIR {
  std::string name;
  VarKind vkind = VarKind::Field;
  int coordIndex = -1;
  std::vector<std::string> tensorIndexNames;
  VarIR(std::string n, VarKind k)
      : ExprIR(Kind::Var), name(std::move(n)), vkind(k) {}
};

struct BinaryIR final : ExprIR {
  std::string op;
  std::unique_ptr<ExprIR> lhs;
  std::unique_ptr<ExprIR> rhs;
  BinaryIR(std::string o, std::unique_ptr<ExprIR> L, std::unique_ptr<ExprIR> R)
      : ExprIR(Kind::Binary), op(std::move(o)), lhs(std::move(L)),
        rhs(std::move(R)) {}
};

struct CallIR final : ExprIR {
  std::string callee;
  std::vector<std::unique_ptr<ExprIR>> args;
  explicit CallIR(std::string c) : ExprIR(Kind::Call), callee(std::move(c)) {}
};

struct EquationIR {
  std::string fieldName;
  std::vector<std::string> indices;
  std::unique_ptr<ExprIR> rhs;
};

struct EvolutionIR {
  std::string name;
  std::vector<EquationIR> equations;
};

struct ModuleIR {
  std::optional<SimulationIR> simulation;
  std::vector<FieldIR> fields;
  std::vector<EvolutionIR> evolutions;
};

} // namespace tensorium::backend


================================================================================
FILE: include/tensorium/Backend/BackendBuilder.hpp
================================================================================

#pragma once
#include "tensorium/AST/AST.hpp"
#include "tensorium/AST/IndexedAST.hpp"
#include "tensorium/Backend/DomainIR.hpp"
#include "tensorium/Sema/Sema.hpp"

namespace tensorium::backend {

class BackendBuilder {
public:
  static ModuleIR build(const Program &prog, SemanticAnalyzer &sem);

private:
  static FieldKind lowerFieldKind(TensorKind k);
};

} // namespace tensorium::backend


================================================================================
FILE: include/tensorium/Backend/IRPrinter.hpp
================================================================================

#pragma once
#include "tensorium/Backend/DomainIR.hpp"
#include <iostream>

namespace tensorium::backend {

inline void printExprIR(const ExprIR *e) {
  if (!e) {
    std::cout << "<null>";
    return;
  }

  switch (e->kind) {
  case ExprIR::Kind::Number: {
    auto *n = static_cast<const NumberIR *>(e);
    std::cout << n->value;
    return;
  }
  case ExprIR::Kind::Var: {
    auto *v = static_cast<const VarIR *>(e);
    std::cout << v->name << "[";
    switch (v->vkind) {
    case VarKind::Field:
      std::cout << "field";
      break;
    case VarKind::Param:
      std::cout << "param";
      break;
    case VarKind::Local:
      std::cout << "local";
      break;
    case VarKind::Coord:
      std::cout << "coord:" << v->coordIndex;
      break;
    }
    std::cout << "]";
    if (!v->tensorIndexNames.empty()) {
      std::cout << "{";
      for (size_t i = 0; i < v->tensorIndexNames.size(); ++i) {
        std::cout << v->tensorIndexNames[i];
        if (i + 1 < v->tensorIndexNames.size())
          std::cout << ",";
      }
      std::cout << "}";
    }
    return;
  }
  case ExprIR::Kind::Binary: {
    auto *b = static_cast<const BinaryIR *>(e);
    std::cout << "(";
    printExprIR(b->lhs.get());
    std::cout << " " << b->op << " ";
    printExprIR(b->rhs.get());
    std::cout << ")";
    return;
  }
  case ExprIR::Kind::Call: {
    auto *c = static_cast<const CallIR *>(e);
    std::cout << c->callee << "(";
    for (size_t i = 0; i < c->args.size(); ++i) {
      printExprIR(c->args[i].get());
      if (i + 1 < c->args.size())
        std::cout << ", ";
    }
    std::cout << ")";
    return;
  }
  }
}

inline void printModuleIR(const ModuleIR &m) {
  std::cout << "BackendModuleIR:\n";

  if (m.simulation) {
    std::cout << "  Simulation:\n";
    std::cout << "    dim = " << m.simulation->dimension << "\n";
    std::cout << "    dt  = " << m.simulation->time.dt << "\n";
  }

  std::cout << "  Fields:\n";
  for (const auto &f : m.fields) {
    std::cout << "    " << f.name << " (up=" << f.up << ",down=" << f.down
              << ")\n";
  }

  std::cout << "  Evolutions:\n";
  for (const auto &evo : m.evolutions) {
    std::cout << "    Evolution " << evo.name << " {\n";
    for (const auto &eq : evo.equations) {
      std::cout << "      dt " << eq.fieldName;
      if (!eq.indices.empty()) {
        std::cout << "[";
        for (size_t i = 0; i < eq.indices.size(); ++i) {
          std::cout << eq.indices[i];
          if (i + 1 < eq.indices.size())
            std::cout << ",";
        }
        std::cout << "]";
      }
      std::cout << " = ";
      printExprIR(eq.rhs.get());
      std::cout << "\n";
    }
    std::cout << "    }\n";
  }
}

} // namespace tensorium::backend


================================================================================
FILE: include/tensorium/AST/Visitor.hpp
================================================================================
#pragma once

namespace tensorium {

struct NumberExpr;
struct VarExpr;
struct BinaryExpr;
struct CallExpr;
struct ParenExpr;
struct IndexedVarExpr;

// class ExprVisitor {
// public:
//   virtual ~ExprVisitor() = default;
//   virtual void visit(const NumberExpr &E) = 0;
//   virtual void visit(const VarExpr &E) = 0;
//   virtual void visit(const BinaryExpr &E) = 0;
//   virtual void visit(const CallExpr &E) = 0;
//   virtual void visit(const ParenExpr &E) = 0;
//   virtual void visit(const IndexedVarExpr &E) = 0;
// };

} // namespace tensorium


================================================================================
FILE: include/tensorium/AST/IndexedAST.hpp
================================================================================
#pragma once
#include "tensorium/AST/AST.hpp"
#include <memory>
#include <string>
#include <vector>

namespace tensorium {

struct IndexedExpr {
  virtual ~IndexedExpr() = default;
};

struct IndexedNumber : IndexedExpr {
  double value;
  explicit IndexedNumber(double v) : value(v) {}
};

enum class IndexedVarKind { Coordinate, Local, Field, Parameter };

struct IndexedVar : IndexedExpr {
  std::string name;
  IndexedVarKind kind;
  TensorKind tensorKind = TensorKind::Scalar;
  int up = 0;
  int down = 0;
  int coordIndex = -1;

  std::vector<int> tensorIndices;
  std::vector<std::string> tensorIndexNames;

  IndexedVar(std::string n, IndexedVarKind k, int cidx = -1)
      : name(std::move(n)), kind(k), coordIndex(cidx) {}
};

struct IndexedBinary : IndexedExpr {
  char op;
  std::unique_ptr<IndexedExpr> lhs;
  std::unique_ptr<IndexedExpr> rhs;
  IndexedBinary(char o, std::unique_ptr<IndexedExpr> L,
                std::unique_ptr<IndexedExpr> R)
      : op(o), lhs(std::move(L)), rhs(std::move(R)) {}
};

struct IndexedCall : IndexedExpr {
  std::string callee;
  std::vector<std::unique_ptr<IndexedExpr>> args;
};

struct IndexedAssignment {
  std::string tensor;
  std::vector<int> indexOffsets;
  std::unique_ptr<IndexedExpr> rhs;
};

struct IndexedMetric {
  std::string name;
  int rank;
  std::vector<std::string> coords;
  std::vector<IndexedAssignment> assignments;
};

struct IndexedEvolutionEq {
  std::string fieldName;
  std::vector<std::string> indices;
  std::unique_ptr<IndexedExpr> rhs;
};

struct IndexedEvolution {
  std::string name;
  std::vector<IndexedEvolutionEq> equations;
  std::vector<IndexedAssignment> temp;
};

} // namespace tensorium


================================================================================
FILE: include/tensorium/AST/ASTPrinter.hpp
================================================================================
#pragma once
#include "tensorium/AST/AST.hpp"

namespace tensorium {
    void printProgram(const Program &prog);
    void printExpr(const Expr *e);
}


================================================================================
FILE: include/tensorium/AST/AST.hpp
================================================================================
#pragma once
#include <memory>
#include <string>
#include <vector>

namespace tensorium {

enum class TensorKind {
  Scalar,
  Vector,
  Covector,
  CovTensor2,
  ConTensor2,
  CovTensor3,
  ConTensor3,
  ConTensor4,
  CovTensor4,
  MixedTensor
};

struct NumberExpr;
struct VarExpr;
struct BinaryExpr;
struct CallExpr;
struct ParenExpr;
struct IndexedVarExpr;

struct ExprVisitor {
  virtual ~ExprVisitor() = default;
  virtual void visit(const NumberExpr &) = 0;
  virtual void visit(const VarExpr &) = 0;
  virtual void visit(const BinaryExpr &) = 0;
  virtual void visit(const CallExpr &) = 0;
  virtual void visit(const ParenExpr &) = 0;
  virtual void visit(const IndexedVarExpr &) = 0;
};

struct Expr {
  virtual ~Expr() = default;
  virtual void accept(ExprVisitor &v) const = 0;
};

struct NumberExpr : Expr {
  double value;
  explicit NumberExpr(double v) : value(v) {}
  void accept(ExprVisitor &v) const override { v.visit(*this); }
};

struct VarExpr : Expr {
  std::string name;
  explicit VarExpr(std::string n) : name(std::move(n)) {}
  void accept(ExprVisitor &v) const override { v.visit(*this); }
};

struct BinaryExpr : Expr {
  std::unique_ptr<Expr> lhs, rhs;
  char op;
  BinaryExpr(std::unique_ptr<Expr> l, char o, std::unique_ptr<Expr> r)
      : lhs(std::move(l)), rhs(std::move(r)), op(o) {}
  void accept(ExprVisitor &v) const override { v.visit(*this); }
};

struct ParenExpr : Expr {
  std::unique_ptr<Expr> inner;
  explicit ParenExpr(std::unique_ptr<Expr> e) : inner(std::move(e)) {}
  void accept(ExprVisitor &v) const override { v.visit(*this); }
};

struct CallExpr : Expr {
  std::string callee;
  std::vector<std::unique_ptr<Expr>> args;
  void accept(ExprVisitor &v) const override { v.visit(*this); }
};

struct IndexedVarExpr : Expr {
  std::string base;
  std::vector<std::string> indices;
  IndexedVarExpr(std::string b, std::vector<std::string> idx)
      : base(std::move(b)), indices(std::move(idx)) {}
  void accept(ExprVisitor &v) const override { v.visit(*this); }
};

// Structures Top-Level
struct TensorAccess {
  std::string base;
  std::vector<std::string> indices;
};
struct Assignment {
  TensorAccess lhs;
  std::unique_ptr<Expr> rhs;
};

struct FieldDecl {
  TensorKind kind;
  std::string name;
  std::vector<std::string> indices;
  int up = 0;
  int down = 0;
};

struct MetricDecl {
  std::string name;
  std::vector<std::string> indices;
  std::vector<Assignment> entries;
};

struct EvolutionEq {
  std::string fieldName;
  std::vector<std::string> indices;
  std::unique_ptr<Expr> rhs;
};

struct EvolutionDecl {
  std::string name;
  std::vector<EvolutionEq> equations;
  std::vector<Assignment> tempAssignments;
};

enum class CoordinateSystem { Cartesian, Spherical, Cylindrical };

enum class TimeIntegrator { Euler, RK3, RK4 };

enum class SpatialScheme { FiniteDifference, Spectral };

enum class DerivativeScheme { Centered, Upwind };

struct TimeConfig {
  double dt = 0.0;
  TimeIntegrator integrator = TimeIntegrator::RK4;
};

struct SpatialConfig {
  SpatialScheme scheme = SpatialScheme::FiniteDifference;
  DerivativeScheme derivative = DerivativeScheme::Centered;
  int order = 2;
};

struct SimulationConfig {
  CoordinateSystem coordinates = CoordinateSystem::Cartesian;
  int dimension = 3;
  std::vector<int> resolution;
  TimeConfig time;
  SpatialConfig spatial;
};

struct Program {
  std::vector<FieldDecl> fields;
  std::vector<MetricDecl> metrics;
  std::vector<EvolutionDecl> evolutions;
  std::unique_ptr<SimulationConfig> simulation;
};
} // namespace tensorium


